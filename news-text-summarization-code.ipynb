{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":13027124,"datasetId":8248395,"databundleVersionId":13701233},{"sourceType":"datasetVersion","sourceId":13008709,"datasetId":8235753,"databundleVersionId":13678441},{"sourceType":"datasetVersion","sourceId":13021557,"datasetId":8244521,"databundleVersionId":13694840},{"sourceType":"datasetVersion","sourceId":10711645,"datasetId":6639109,"databundleVersionId":11060262},{"sourceType":"datasetVersion","sourceId":13069710,"datasetId":8277211,"databundleVersionId":13749841}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================\n# BBC News — cleaned HF dataset (capped at 2200 rows)\n# =========================\n# Preprocessing:\n# - Unicode normalization & basic cleaning\n# - Lowercasing\n# - Duplicate removal (exact duplicates on article+highlights)\n# - Tokenization (NLTK word_tokenize)\n# - Stopword removal (NLTK)\n# - Lemmatization (WordNet, POS-aware)\n# Columns preserved: 'article', 'highlights'\n# Extra columns (for inspection): 'article_tokens', 'highlights_tokens'\n# =========================\n\nimport os\nimport re\nimport unicodedata\nimport warnings\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# HuggingFace datasets\nfrom datasets import Dataset, DatasetDict\n\n# NLTK setup\nimport nltk\nfor pkg in [\n    \"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\",\n    \"averaged_perceptron_tagger\", \"averaged_perceptron_tagger_eng\"\n]:\n    try:\n        nltk.download(pkg, quiet=True)\n    except Exception as e:\n        warnings.warn(f\"NLTK download warning for {pkg}: {e}\")\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag, word_tokenize\n\n# -----------------------------\n# Paths / I/O\n# -----------------------------\nCSV_PATH = \"/kaggle/input/bbc-news-dataset/bbc_news_summary.csv\"\n\n# -----------------------------\n# Helpers\n# -----------------------------\n_STOPWORDS = set(stopwords.words(\"english\"))\n_LEMMA = WordNetLemmatizer()\n\ndef _to_wordnet_pos(tag: str):\n    if tag.startswith('J'): return 'a'\n    if tag.startswith('V'): return 'v'\n    if tag.startswith('N'): return 'n'\n    if tag.startswith('R'): return 'r'\n    return 'n'\n\ndef _normalize_unicode(s: str) -> str:\n    s = unicodedata.normalize(\"NFKC\", s)\n    s = s.replace(\"\\u200b\", \" \").replace(\"\\ufeff\", \" \")\n    s = re.sub(r\"[ \\t]+\", \" \", s)\n    s = re.sub(r\"\\s*\\n\\s*\", \"\\n\", s)\n    return s.strip()\n\ndef _basic_clean(s: str) -> str:\n    s = s.lower()\n    s = (s.replace(\"“\", '\"').replace(\"”\", '\"')\n           .replace(\"’\", \"'\").replace(\"‘\", \"'\")\n           .replace(\"—\", \"-\").replace(\"–\", \"-\"))\n    # drop control chars except newlines\n    s = \"\".join(ch for ch in s if ch == \"\\n\" or unicodedata.category(ch)[0] != \"C\")\n    return s.strip()\n\ndef _tokenize_lemmatize_stop(text: str):\n    tokens = word_tokenize(text)\n    # Robust POS tag fallback\n    try:\n        tagged = pos_tag(tokens, lang=\"eng\")\n    except LookupError:\n        try:\n            nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n            tagged = pos_tag(tokens, lang=\"eng\")\n        except Exception:\n            nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n            tagged = pos_tag(tokens)\n\n    cleaned_tokens = []\n    for tok, tg in tagged:\n        # keep only alnum-ish tokens\n        if not re.search(r\"[A-Za-z0-9]\", tok):\n            continue\n        if tok in _STOPWORDS:\n            continue\n        wn_pos = _to_wordnet_pos(tg)\n        lemma = _LEMMA.lemmatize(tok, pos=wn_pos)\n        cleaned_tokens.append(lemma)\n\n    cleaned_text = \" \".join(cleaned_tokens)\n    return cleaned_text, cleaned_tokens\n\ndef preprocess_text(text: str):\n    if not isinstance(text, str):\n        text = \"\" if pd.isna(text) else str(text)\n    text = _normalize_unicode(text)\n    text = _basic_clean(text)\n    cleaned_text, tokens = _tokenize_lemmatize_stop(text)\n    # if aggressive cleaning empties text, fall back to lightly cleaned text\n    if not cleaned_text:\n        cleaned_text = text\n        tokens = word_tokenize(text) if text else []\n    return cleaned_text, tokens\n\n# -----------------------------\n# Load & validate\n# -----------------------------\ndf = pd.read_csv(CSV_PATH)\n\n# Fix column names if needed\ndf = df.rename(columns={\"Articles\": \"article\", \"Summaries\": \"highlights\"})\nexpected_cols = {\"article\", \"highlights\"}\nmissing = expected_cols - set(df.columns)\nif missing:\n    raise ValueError(f\"CSV missing required columns {missing}. Found: {list(df.columns)}\")\n\n# -----------------------------\n# Clean & filter (pre-limit)\n# -----------------------------\nfor col in [\"article\", \"highlights\"]:\n    df[col] = df[col].astype(str).map(lambda s: \" \".join(str(s).split()))\n\ndf.dropna(subset=[\"article\", \"highlights\"], inplace=True)\ndf = df[df[\"article\"].str.len() > 30]\ndf = df[df[\"highlights\"].str.len() > 10]\n\n# Drop exact duplicates\nbefore = len(df)\ndf = df.drop_duplicates(subset=[\"article\", \"highlights\"]).reset_index(drop=True)\nafter = len(df)\nprint(f\"[INFO] Dropped {before - after} duplicate rows.\")\nprint(f\"[INFO] Rows after basic cleaning & dedup: {len(df)}\")\n\n# -----------------------------\n# Hard cap at 2200 rows (AFTER cleaning, BEFORE preprocessing)\n# -----------------------------\nTARGET_N = 2200\nn_available = len(df)\n\nif n_available >= TARGET_N:\n    df = df.sample(n=TARGET_N, random_state=42).reset_index(drop=True)\nelse:\n    # shuffle all if fewer than target\n    df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\nprint(f\"[INFO] Using {len(df)} rows for preprocessing (target {TARGET_N}).\")\n\n# -----------------------------\n# Preprocess\n# -----------------------------\narticles_clean, articles_tokens = [], []\nhighlights_clean, highlights_tokens = [], []\n\nprint(\"[INFO] Preprocessing texts...\")\nfor a, h in tqdm(zip(df[\"article\"].astype(str), df[\"highlights\"].astype(str)),\n                 total=len(df), leave=False):\n    a_clean, a_tok = preprocess_text(a)\n    h_clean, h_tok = preprocess_text(h)\n    articles_clean.append(a_clean)\n    articles_tokens.append(a_tok)\n    highlights_clean.append(h_clean)\n    highlights_tokens.append(h_tok)\n\ndf[\"article\"] = articles_clean\ndf[\"highlights\"] = highlights_clean\ndf[\"article_tokens\"] = articles_tokens\ndf[\"highlights_tokens\"] = highlights_tokens\n\n# Ensure non-empty after preprocessing\ndf = df[(df[\"article\"].str.len() > 0) & (df[\"highlights\"].str.len() > 0)].reset_index(drop=True)\nprint(f\"[INFO] Final row count after preprocessing: {len(df)}\")\n\n# -----------------------------\n# HuggingFace Dataset\n# -----------------------------\nhf_all = Dataset.from_pandas(df)\nsplit = hf_all.train_test_split(test_size=0.2, seed=42)\ndataset = DatasetDict({\"train\": split[\"train\"], \"test\": split[\"test\"]})\n\nprint(dataset)\nprint(f\"Features: {dataset['train'].column_names}\")\nprint(\"=\" * 100)\n\n# -----------------------------\n# Show samples\n# -----------------------------\nfor i in range(min(3, len(dataset[\"train\"]))):\n    sample = dataset[\"train\"][i]\n    art = sample[\"article\"]; summ = sample[\"highlights\"]\n    print(f\"Sample {i+1}\")\n    print(\"-\" * 100)\n    print(f\"Article (len {len(art)}):\\n{art[:1500]}{'...' if len(art) > 1500 else ''}\")\n    print(\"-\" * 100)\n    print(f\"Summary (len {len(summ)}):\\n{summ}\")\n    print(\"-\" * 100)\n    print(f\"Article tokens preview: {sample.get('article_tokens', [])[:30]}\")\n    print(f\"Summary tokens preview: {sample.get('highlights_tokens', [])[:30]}\")\n    print(\"=\" * 100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:43:01.203878Z","iopub.execute_input":"2025-09-15T17:43:01.204159Z","iopub.status.idle":"2025-09-15T17:44:09.135276Z","shell.execute_reply.started":"2025-09-15T17:43:01.204136Z","shell.execute_reply":"2025-09-15T17:44:09.134203Z"}},"outputs":[{"name":"stdout","text":"[INFO] Dropped 98 duplicate rows.\n[INFO] Rows after basic cleaning & dedup: 2126\n[INFO] Using 2126 rows for preprocessing (target 2200).\n[INFO] Preprocessing texts...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2126 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[INFO] Final row count after preprocessing: 2126\nDatasetDict({\n    train: Dataset({\n        features: ['File_path', 'article', 'highlights', 'article_tokens', 'highlights_tokens'],\n        num_rows: 1700\n    })\n    test: Dataset({\n        features: ['File_path', 'article', 'highlights', 'article_tokens', 'highlights_tokens'],\n        num_rows: 426\n    })\n})\nFeatures: ['File_path', 'article', 'highlights', 'article_tokens', 'highlights_tokens']\n====================================================================================================\nSample 1\n----------------------------------------------------------------------------------------------------\nArticle (len 907):\nad firm wpp 's profit surge 15 uk advertising giant wpp post larger-than-expected annual profit predict outperform market 2005 pre-tax profit rise 15 year ago reach £546m 1.04bn ahead average analyst forecast £532m revenue £4.3bn firm 's operating margin 14.1 say could reach 14.8 2006. year wpp buy u rival grey global create giant big enough rival sector leader omnicom chief executive martin sorrell friday tell reuters news agency wpp submit proposal united business medium 's nop world market research unit analyst say unit sell could sell £350m wpp recent year also buy firm ogilvy mather cordiant communication also include firm young rubicam j walter thompson event olympics help boost wpp 's profit 2004. company say u congressional election fifa world cup likely present advertising opportunity near future long-term outlook look favourable medium technology development strength u economy wpp say\n----------------------------------------------------------------------------------------------------\nSummary (len 438):\nwpp recent year also buy firm ogilvy mather cordiant communications.uk advertising giant wpp post larger-than-expected annual profit predict outperform market 2005.pre-tax profit rise 15 year ago reach £546m 1.04bn ahead average analyst forecast £532m.during year wpp buy u rival grey global create giant big enough rival sector leader omnicom.the long-term outlook look favourable medium technology development strength u economy wpp say\n----------------------------------------------------------------------------------------------------\nArticle tokens preview: ['ad', 'firm', 'wpp', \"'s\", 'profit', 'surge', '15', 'uk', 'advertising', 'giant', 'wpp', 'post', 'larger-than-expected', 'annual', 'profit', 'predict', 'outperform', 'market', '2005', 'pre-tax', 'profit', 'rise', '15', 'year', 'ago', 'reach', '£546m', '1.04bn', 'ahead', 'average']\nSummary tokens preview: ['wpp', 'recent', 'year', 'also', 'buy', 'firm', 'ogilvy', 'mather', 'cordiant', 'communications.uk', 'advertising', 'giant', 'wpp', 'post', 'larger-than-expected', 'annual', 'profit', 'predict', 'outperform', 'market', '2005.pre-tax', 'profit', 'rise', '15', 'year', 'ago', 'reach', '£546m', '1.04bn', 'ahead']\n====================================================================================================\nSample 2\n----------------------------------------------------------------------------------------------------\nArticle (len 1080):\nturkey-iran mobile deal 'at risk turkey 's investment iran 's mobile industry look set scrap big mobile firm saw investment slash mp iran 's parliament vote large majority cut turkcell 's stake new mobile network 70 49 move justify national security ground follow early vote mp give veto foreign investment turkcell say decision increase risk attach project although company 's statement say would continue monitor development observer say think turkcell set pull 3bn deal possibility carry project next zero say atinc ozkan analyst finans investment istanbul turkcell back mtn south african firm lose original tender may well back running company say prepare accept minority stake iran award mobile deal turkcell 's mobile deal second turkish investment iran run trouble turkish-austrian consortium tav choose build run tehran 's new imam khomeini international airport army close hour open may 2004. case justification national security amid allegation turkish firm close israel hardline posture take parliament dominate religious conservative could yet impact inward investment\n----------------------------------------------------------------------------------------------------\nSummary (len 437):\nturkcell 's mobile deal second turkish investment iran run trouble.the company say prepare accept minority stake iran award mobile deal.turkey 's investment iran 's mobile industry look set scrap big mobile firm saw investment slash mps.although company 's statement say would continue monitor development observer say think turkcell set pull 3bn deal.iran 's parliament vote large majority cut turkcell 's stake new mobile network 70 49\n----------------------------------------------------------------------------------------------------\nArticle tokens preview: ['turkey-iran', 'mobile', 'deal', \"'at\", 'risk', 'turkey', \"'s\", 'investment', 'iran', \"'s\", 'mobile', 'industry', 'look', 'set', 'scrap', 'big', 'mobile', 'firm', 'saw', 'investment', 'slash', 'mp', 'iran', \"'s\", 'parliament', 'vote', 'large', 'majority', 'cut', 'turkcell']\nSummary tokens preview: ['turkcell', \"'s\", 'mobile', 'deal', 'second', 'turkish', 'investment', 'iran', 'run', 'trouble.the', 'company', 'say', 'prepare', 'accept', 'minority', 'stake', 'iran', 'award', 'mobile', 'deal.turkey', \"'s\", 'investment', 'iran', \"'s\", 'mobile', 'industry', 'look', 'set', 'scrap', 'big']\n====================================================================================================\nSample 3\n----------------------------------------------------------------------------------------------------\nArticle (len 1565):\n'brainwave cap control computer team u researcher show control device brain step closer four people two partly paralyse wheelchair user successfully move computer cursor wear cap 64 electrode previous research show monkey control computer electrode implant brain new york team report finding proceeding national academy science result show people learn use scalp-recorded electroencephalogram rhythm control rapid accurate movement cursor two direction say jonathan wolpaw dennis mcfarlane research team new york state department health state university new york albany say research another step towards people control wheelchair electronic device thought four people face large video screen wear special cap mean surgery implantation need brain activity produce electrical signal read electrode complex algorithm translate signal instruction direct computer brain activity require use nerve muscle people stroke spinal cord injury could use cap effectively impressive non-invasive multidimensional control achieve present study suggest non-invasive brain control interface could support clinically useful operation robotic arm motorised wheelchair neuroprosthesis say researcher four volunteer also show could get good control cursor time try although two partially-paralysed people perform well overall researcher say could brain used adapt simply motivated first time researcher sort success brain-control experiment team use eye motion record technique earlier year team mit medium labs europe de...\n----------------------------------------------------------------------------------------------------\nSummary (len 732):\nalthough two partially-paralysed people perform well overall researcher say could brain used adapt simply motivated.a team u researcher show control device brain step closer.earlier year team mit medium labs europe demonstrate wireless cap read brain wave control computer character.four people two partly paralyse wheelchair user successfully move computer cursor wear cap 64 electrodes.previous research show monkey control computer electrode implant brain.the research team new york state department health state university new york albany say research another step towards people control wheelchair electronic device thought.such brain activity require use nerve muscle people stroke spinal cord injury could use cap effectively\n----------------------------------------------------------------------------------------------------\nArticle tokens preview: [\"'brainwave\", 'cap', 'control', 'computer', 'team', 'u', 'researcher', 'show', 'control', 'device', 'brain', 'step', 'closer', 'four', 'people', 'two', 'partly', 'paralyse', 'wheelchair', 'user', 'successfully', 'move', 'computer', 'cursor', 'wear', 'cap', '64', 'electrode', 'previous', 'research']\nSummary tokens preview: ['although', 'two', 'partially-paralysed', 'people', 'perform', 'well', 'overall', 'researcher', 'say', 'could', 'brain', 'used', 'adapt', 'simply', 'motivated.a', 'team', 'u', 'researcher', 'show', 'control', 'device', 'brain', 'step', 'closer.earlier', 'year', 'team', 'mit', 'medium', 'labs', 'europe']\n====================================================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# # =========================\n# # CNN/DailyMail — 2,200 rows with Train/Val/Test (80/10/10)\n# # =========================\n\n# import pandas as pd\n# from datasets import Dataset, DatasetDict\n\n# # --- Load CSV ---\n# df = pd.read_csv(\"/kaggle/input/cnn-dataser/cnn_dailymail_summary.csv\")\n\n# # If your file has 'Articles' and 'Summaries', rename to expected schema\n# df = df.rename(columns={\"Articles\": \"article\", \"Summaries\": \"highlights\"})\n\n# # --- Basic filtering ---\n# df.dropna(subset=[\"article\", \"highlights\"], inplace=True)\n# df = df[df[\"article\"].astype(str).str.len() > 30]\n# df = df[df[\"highlights\"].astype(str).str.len() > 10]\n\n# # --- Select exactly 2,200 rows (reproducible) ---\n# if len(df) < 2200:\n#     raise ValueError(f\"Dataset has {len(df)} usable rows after filtering; need at least 2200.\")\n# df = df.sample(n=2200, random_state=42).reset_index(drop=True)\n\n# # --- Build Hugging Face dataset & 80/10/10 split ---\n# hf_all = Dataset.from_pandas(df.reset_index(drop=True))\n# tmp = hf_all.train_test_split(test_size=0.20, seed=42)            # 80% train, 20% temp\n# val_test = tmp[\"test\"].train_test_split(test_size=0.50, seed=42)  # split temp -> 10% val, 10% test\n\n# dataset = DatasetDict({\n#     \"train\": tmp[\"train\"],\n#     \"validation\": val_test[\"train\"],\n#     \"test\": val_test[\"test\"]\n# })\n\n# # --- Inspect sizes and features ---\n# print(dataset)\n# for split in [\"train\", \"validation\", \"test\"]:\n#     print(f\"{split}: {len(dataset[split])} rows\")\n# print(\"Features:\", dataset[\"train\"].column_names)\n# print(\"=\" * 100)\n\n# # --- Peek a few training samples ---\n# for i in range(min(3, len(dataset[\"train\"]))):\n#     s = dataset[\"train\"][i]\n#     article = str(s[\"article\"])\n#     print(f\"Sample {i+1}\")\n#     print(\"-\" * 80)\n#     print(f\"Article ({len(article)} chars):\\n{article[:1500]}{'...' if len(article) > 1500 else ''}\")\n#     print(\"-\" * 80)\n#     print(f\"Summary:\\n{s['highlights']}\")\n#     print(\"=\" * 100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:44:09.136846Z","iopub.execute_input":"2025-09-15T17:44:09.137108Z","iopub.status.idle":"2025-09-15T17:44:09.142453Z","shell.execute_reply.started":"2025-09-15T17:44:09.137085Z","shell.execute_reply":"2025-09-15T17:44:09.141771Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Extract a sample article from your custom dataset for testing generation\nsample_text = dataset['test'][1]['article'][:5000]  # use test set to simulate inference\nprint(\"Sample article:\\n\")\nprint(sample_text)\nprint(\"\\n\" + \"=\"*100 + \"\\n\")\n\n# Initialize an empty dictionary to store model summaries\nsummaries = {}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:44:09.143758Z","iopub.execute_input":"2025-09-15T17:44:09.144007Z","iopub.status.idle":"2025-09-15T17:44:09.168060Z","shell.execute_reply.started":"2025-09-15T17:44:09.143986Z","shell.execute_reply":"2025-09-15T17:44:09.167006Z"}},"outputs":[{"name":"stdout","text":"Sample article:\n\nbroadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord bt 95 uk home business receive broadband phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk\n\n====================================================================================================\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import nltk \nfrom nltk.tokenize import sent_tokenize\nnltk.download(\"punkt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:44:09.170557Z","iopub.execute_input":"2025-09-15T17:44:09.170875Z","iopub.status.idle":"2025-09-15T17:44:09.200338Z","shell.execute_reply.started":"2025-09-15T17:44:09.170825Z","shell.execute_reply":"2025-09-15T17:44:09.199420Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## *Extract the first three sentences as a summary*","metadata":{}},{"cell_type":"code","source":"# Define a baseline summarizer: first 3 sentences of the article\ndef three_sentence_summary(text):\n    return \"\\n\".join(sent_tokenize(text)[:3])\n\n# Generate baseline summary for your sample article\nsummaries[\"baseline\"] = three_sentence_summary(sample_text)\n\n# Show result\nprint(\"=== BASELINE SUMMARY ===\\n\")\nprint(summaries[\"baseline\"])\nprint(\"\\n\" + \"=\"*100 + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:44:09.201326Z","iopub.execute_input":"2025-09-15T17:44:09.201729Z","iopub.status.idle":"2025-09-15T17:44:09.220397Z","shell.execute_reply.started":"2025-09-15T17:44:09.201692Z","shell.execute_reply":"2025-09-15T17:44:09.219390Z"}},"outputs":[{"name":"stdout","text":"=== BASELINE SUMMARY ===\n\nbroadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord bt 95 uk home business receive broadband phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk\n\n====================================================================================================\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**    XLNET**","metadata":{}},{"cell_type":"code","source":"# ---------------- XLNet ----------------\nfrom transformers import pipeline\nxlnet_pipe = pipeline(\"text-generation\", model=\"xlnet-base-cased\", device=0)\nsummaries[\"xlnet\"] = xlnet_pipe(sample_text,\n    max_length=256,\n    batch_size=128,\n    top_p=0.9,\n    repetition_penalty=1.2\n)[0][\"generated_text\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:44:09.221392Z","iopub.execute_input":"2025-09-15T17:44:09.221708Z","iopub.status.idle":"2025-09-15T17:46:08.071156Z","shell.execute_reply.started":"2025-09-15T17:44:09.221681Z","shell.execute_reply":"2025-09-15T17:46:08.070294Z"}},"outputs":[{"name":"stderr","text":"2025-09-15 17:44:23.460839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757958263.716458      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757958263.794023      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f6ee70a0a84c9ab3f7d3226a873ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"426855d1f1934b2295b4d9846135edb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"786e177d9034438f9dbec0b09ebea7ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5825b0e1cf60404485da28f50f297dc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da4082b10d20438492e838b7c5a4ecd5"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"468ecc60080847e29dbc5117d8d1322a"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nBoth `max_new_tokens` (=256) and `max_length`(=421) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nThis is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (-1). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## GPT-2","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nset_seed(42)\npipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\ngpt2_query = sample_text + \"\\nTL;DR:\\n\"\npipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\nsummaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query):]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:46:08.072290Z","iopub.execute_input":"2025-09-15T17:46:08.073206Z","iopub.status.idle":"2025-09-15T17:48:57.020042Z","shell.execute_reply.started":"2025-09-15T17:46:08.073166Z","shell.execute_reply":"2025-09-15T17:48:57.018563Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7848384d6f8648cb8f5f2eee69876cac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d571bd403964af79170459874c8359e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0544bb42fc842f5921eb918b910ee35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40cc290388ee4bcbae018452927ae50c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d65f0b0e87474b32bed4cae35f10180a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543a296e73194ed694b9428b3db1c644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c455597b1a4f968b25c761c03c5a17"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nBoth `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## T-5","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nfrom nltk.tokenize import sent_tokenize\nimport torch, nltk\n\n# ensure sentence splitter\ntry:\n    nltk.data.find(\"tokenizers/punkt\")\nexcept LookupError:\n    nltk.download(\"punkt\")\n\ndevice_id = 0 if torch.cuda.is_available() else -1\nmodel_id = \"t5-large\"\n\ntok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nmdl = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n\nt5_pipe = pipeline(\"summarization\", model=mdl, tokenizer=tok, device=device_id)\n\n# --- helper for long inputs (T5 encoder ~512 tokens) ---\ndef summarize_with_t5_map_reduce(\n    text,\n    pipe=t5_pipe,\n    tok=tok,\n    max_input_tokens=512,     # T5-large encoder window\n    chunk_buffer=32,          # keep some room for special tokens\n    stride_tokens=128,        # overlap to preserve context\n    num_beams=4,\n    min_length=30,\n    max_length=70,\n    no_repeat_ngram_size=2,\n    length_penalty=1.0,\n):\n    # encode once (ids length ~= tokens)\n    ids = tok.encode(text, add_special_tokens=False)\n    chunk_len = max_input_tokens - chunk_buffer\n\n    gen_kwargs = dict(\n        num_beams=num_beams,\n        min_length=min_length,\n        max_length=max_length,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        length_penalty=length_penalty,\n        truncation=True,\n        clean_up_tokenization_spaces=True,\n    )\n\n    # short enough: single pass\n    if len(ids) <= chunk_len:\n        return pipe(text, **gen_kwargs)[0][\"summary_text\"].strip()\n\n    # map-reduce: chunk -> summarize parts -> summarize the join\n    chunks, i = [], 0\n    while i < len(ids):\n        window = ids[i:i+chunk_len]\n        chunks.append(tok.decode(window, skip_special_tokens=True))\n        if i + chunk_len >= len(ids):\n            break\n        i += max(1, chunk_len - stride_tokens)\n\n    partials = [pipe(part, **gen_kwargs)[0][\"summary_text\"].strip() for part in chunks]\n    merged = \" \".join(partials)\n    final = pipe(merged, **gen_kwargs)[0][\"summary_text\"].strip()\n    return final\n\n# Use it\n# sample_text = dataset[\"test\"][0][\"article\"]\nt5_summary = summarize_with_t5_map_reduce(sample_text)\nsummaries[\"t5\"] = \"\\n\".join(sent_tokenize(t5_summary))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:48:57.021734Z","iopub.execute_input":"2025-09-15T17:48:57.022131Z","iopub.status.idle":"2025-09-15T17:49:47.140893Z","shell.execute_reply.started":"2025-09-15T17:48:57.022080Z","shell.execute_reply":"2025-09-15T17:49:47.140094Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d67e617b785547ecb26e1f383289e4ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3624c249024feaa2afb19e3c8befe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c437dbbb55b64701af7f6916d8d0d534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1102d352edfb495592f18954f6116154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124c8ca0a69344f29a108f6858a3943d"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nBoth `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"**BigBird**","metadata":{}},{"cell_type":"code","source":"# BigBird-Pegasus long-doc summarization (drop-in)\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nfrom nltk.tokenize import sent_tokenize\n\n# 1) Pick a BigBird-Pegasus checkpoint\n#   Common long-doc options: \"google/bigbird-pegasus-large-arxiv\", \"google/bigbird-pegasus-large-pubmed\"\n#   For general/news, arXiv works surprisingly well in practice for long inputs:\nbbp_model_id = \"google/bigbird-pegasus-large-arxiv\"\n\n# 2) Load model + tokenizer\ntokenizer_bbp = AutoTokenizer.from_pretrained(bbp_model_id, use_fast=True)\nmodel_bbp = AutoModelForSeq2SeqLM.from_pretrained(bbp_model_id)\n\n# 3) HF pipeline (GPU = 0 like your T5 example)\nbbp_pipe = pipeline(\"summarization\", model=model_bbp, tokenizer=tokenizer_bbp, device=0)\n\n# 4) Helper: chunk long texts safely and do map-reduce summarization\ndef summarize_long_with_bbp(\n    text: str,\n    pipe,\n    tokenizer,\n    max_input_tokens: int = 4096,   # BigBird-Pegasus context\n    chunk_buffer: int = 64,         # reserve for special tokens\n    stride_tokens: int = 256,       # overlap to preserve context\n    reduce_once: bool = True,\n    **gen_kwargs\n) -> str:\n    \"\"\"\n    Splits very long text into token windows, summarizes each, then (optionally) summarizes\n    the concatenated chunk summaries once more for a concise final output.\n    \"\"\"\n    # sensible defaults for news summaries; tweak as desired\n    if not gen_kwargs:\n        gen_kwargs = dict(\n            num_beams=4,\n            length_penalty=1.0,\n            no_repeat_ngram_size=3,\n            min_length=80,\n            max_length=220\n        )\n\n    # Encode once to tokens so we chunk by tokens (not characters)\n    ids = tokenizer.encode(text, add_special_tokens=False)\n    chunk_len = max_input_tokens - chunk_buffer\n    if chunk_len <= 0:\n        raise ValueError(\"chunk_len <= 0; lower chunk_buffer or raise max_input_tokens.\")\n\n    # If short enough, just summarize directly\n    if len(ids) <= chunk_len:\n        out = pipe(text, **gen_kwargs)[0][\"summary_text\"]\n        return out.strip()\n\n    # Otherwise, sliding windows with stride\n    chunks = []\n    i = 0\n    while i < len(ids):\n        window = ids[i : i + chunk_len]\n        chunks.append(tokenizer.decode(window, skip_special_tokens=True))\n        if i + chunk_len >= len(ids):\n            break\n        i += max(1, chunk_len - stride_tokens)\n\n    # Summarize each chunk\n    part_summaries = []\n    for part in chunks:\n        s = pipe(part, **gen_kwargs)[0][\"summary_text\"]\n        part_summaries.append(s.strip())\n\n    if not reduce_once:\n        # Return concatenated parts if you want the long form\n        return \"\\n\".join(part_summaries)\n\n    # Reduce step: summarize the concatenated chunk summaries\n    joined = \" \".join(part_summaries)\n    final = pipe(joined, **gen_kwargs)[0][\"summary_text\"]\n    return final.strip()\n\n# 5) Use it on your sample_text and store in your summaries dict\npipe_output_bbp = summarize_long_with_bbp(sample_text, bbp_pipe, tokenizer_bbp)\nsummaries[\"bigbird_pegasus\"] = \"\\n\".join(sent_tokenize(pipe_output_bbp))\n\nprint(\"BigBird-Pegasus summary:\\n\", summaries[\"bigbird_pegasus\"][:500], \"...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:49:47.142050Z","iopub.execute_input":"2025-09-15T17:49:47.142416Z","iopub.status.idle":"2025-09-15T17:51:40.550024Z","shell.execute_reply.started":"2025-09-15T17:49:47.142389Z","shell.execute_reply":"2025-09-15T17:51:40.548950Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94bea25546cd4170b957127f25678112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dcafe95cffd4af391b9425af2e1e547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdf8851a0a5f49a792130a6646aa5ac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3230e2961fe41b7ac12b3a56a949d09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28c68f77d874931bbd638ee979078e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a809bc3f3c3e41178f0b7e07f469c8fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d3583864f34715b24659fc66ece229"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/232 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17a5acc127e48288cb60cbd8b6a6f07"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nYour max_length is set to 220, but your input_length is only 192. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=96)\nAttention type 'block_sparse' is not possible if sequence_length: 192 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n","output_type":"stream"},{"name":"stdout","text":"BigBird-Pegasus summary:\n the number of cable broadband customers in the united kingdom has grown to 1.7 million by end of 2006 , an increase of a factor of three compared to the same period a year ago .<n> cable broadband market penetration in the uk is one of the highest in the world , followed by the arab world .\nin the first quarter of 2007 , the company s revenue and profit increased more than twofold and three times , respectively , over the first three months of the year , as reported by the company in its annual  ...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## BART","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nfrom nltk.tokenize import sent_tokenize\nimport torch, nltk\n\n# make sure sentence splitter is available\ntry:\n    nltk.data.find(\"tokenizers/punkt\")\nexcept LookupError:\n    nltk.download(\"punkt\")\n\ndevice_id = 0 if torch.cuda.is_available() else -1\n\nbart_pipe = pipeline(\n    \"summarization\",\n    model=\"facebook/bart-large-cnn\",\n    device=device_id\n)\n\n# Your sample text here\n# sample_text = dataset[\"test\"][0][\"article\"]\n\n# Hyperparameters (your spec + sensible defaults)\nbart_output = bart_pipe(\n    sample_text,\n    truncation=True,         # ensure input is cut to model's 1024-token limit\n    max_length=70,           # generated summary target length\n    min_length=30,           # helps avoid overly short summaries\n    num_beams=4,             # better quality than greedy; modest compute\n    no_repeat_ngram_size=3,  # reduce repetition\n    early_stopping=True\n    # note: batch_size=16 only matters when passing a list of texts, e.g. bart_pipe([text1, text2], batch_size=16)\n)\n\nsummaries[\"bart\"] = \"\\n\".join(sent_tokenize(bart_output[0][\"summary_text\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:51:40.553318Z","iopub.execute_input":"2025-09-15T17:51:40.553742Z","iopub.status.idle":"2025-09-15T17:52:08.342180Z","shell.execute_reply.started":"2025-09-15T17:51:40.553713Z","shell.execute_reply":"2025-09-15T17:52:08.340856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46bdeff1b68e4c388ec3e023e3f46f42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b429ad4943464f875762886ab592e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5139b00f90498993a9dfd3e329ddb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2debefa190e44a8486d3c3961fd5de35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6110d9664144442a1075dcd5e2ca1e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89924ba3c1f541d9ab825028169a5592"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**DistilBERT**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nfrom nltk.tokenize import sent_tokenize\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndistilbart_id = \"sshleifer/distilbart-cnn-12-6\"  # distilled BART for summarization\ndb_tok = AutoTokenizer.from_pretrained(distilbart_id, use_fast=True)\ndb_model = AutoModelForSeq2SeqLM.from_pretrained(distilbart_id)\ndb_pipe = pipeline(\"summarization\", model=db_model, tokenizer=db_tok, device=0)\n\ndef summarize_long_with_distilbart(\n    text, pipe=db_pipe, tok=db_tok,\n    max_input_tokens=1024, chunk_buffer=32, stride_tokens=128,\n    **gen_kwargs\n):\n    if not gen_kwargs:\n        gen_kwargs = dict(num_beams=4, length_penalty=1.0, no_repeat_ngram_size=3,\n                          min_length=60, max_length=180)\n\n    ids = tok.encode(text, add_special_tokens=False)\n    chunk_len = max_input_tokens - chunk_buffer\n    if len(ids) <= chunk_len:\n        return pipe(text, **gen_kwargs)[0][\"summary_text\"].strip()\n\n    # map-reduce: chunk -> summarize parts -> summarize the join\n    chunks, i = [], 0\n    while i < len(ids):\n        window = ids[i:i+chunk_len]\n        chunks.append(tok.decode(window, skip_special_tokens=True))\n        if i + chunk_len >= len(ids): break\n        i += max(1, chunk_len - stride_tokens)\n\n    parts = [pipe(part, **gen_kwargs)[0][\"summary_text\"].strip() for part in chunks]\n    final = pipe(\" \".join(parts), **gen_kwargs)[0][\"summary_text\"].strip()\n    return final\n\n# Use it like your T5 pipeline:\ndb_out = summarize_long_with_distilbart(sample_text)\nsummaries[\"distilbart\"] = \"\\n\".join(sent_tokenize(db_out))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:52:08.343211Z","iopub.execute_input":"2025-09-15T17:52:08.343630Z","iopub.status.idle":"2025-09-15T17:52:32.679471Z","shell.execute_reply.started":"2025-09-15T17:52:08.343594Z","shell.execute_reply":"2025-09-15T17:52:32.677372Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a568478bfb57410f980065da2f4bcca5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb78074fd99c40f9bc08cb72cf701531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed60247c8e0a43c08b4305ff3e415e28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f7c929be874d0fa0f86f150b802af9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1864443a288540438f6ad4db6b199174"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c80e58470664542a75e53e8a6cb330c"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## PEGASUS","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nfrom nltk.tokenize import sent_tokenize\n\n# Pegasus summarization pipeline\npegasus_id = \"google/pegasus-cnn_dailymail\"\npegasus_pipe = pipeline(\"summarization\", model=pegasus_id, device=0)\n\n# Generate summary with hyperparameters\npegasus_out = pegasus_pipe(\n    sample_text,\n    max_length=200,         # Generated summary length cap\n    min_length=50,          # Minimum length to avoid too short summaries\n    truncation=True,        # Truncate input at 1024 tokens\n    early_stopping=True,    # Stop when EOS is likely\n    num_beams=4,            # Beam search (can adjust if needed)\n    no_repeat_ngram_size=3, # Prevent repetition\n    batch_size=2            # Matches your table\n)\n\n# Clean formatting\nsummaries[\"pegasus\"] = \"\\n\".join(sent_tokenize(\n    pegasus_out[0][\"summary_text\"].replace(\" .<n>\", \". \")\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:52:32.681410Z","iopub.execute_input":"2025-09-15T17:52:32.682041Z","iopub.status.idle":"2025-09-15T17:53:32.499739Z","shell.execute_reply.started":"2025-09-15T17:52:32.681986Z","shell.execute_reply":"2025-09-15T17:53:32.498730Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1828f0ac77df46d0949abe44a32a25bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05f619bea68043c788807311e6e39c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b76a0d00304e369836bf3c93b81f31"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376fbe10dc4546e384e3118f91d5f761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7cd5a101bb4784a49e109113a2c51d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5deb67a5ef5f4217a9563e3d33eee3aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d5a1fc94ddd45ecaa3d63f2410ee826"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nYour max_length is set to 200, but your input_length is only 192. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=96)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import pipeline\nfrom nltk.tokenize import sent_tokenize\n\n# Load the LED summarization model\nled_id = \"allenai/led-base-16384\"\nled_pipe = pipeline(\"summarization\", model=led_id, device=0)\n\n# Generate summary with hyperparameters\nled_out = led_pipe(\n    sample_text,\n    max_length=150,         # summary max length\n    min_length=50,          # reasonable lower bound\n    truncation=True,        # truncate input to 16384 tokens\n    early_stopping=True,    # stop when EOS token predicted\n    num_beams=4,            # helps improve quality\n    no_repeat_ngram_size=3, # avoid repetition\n    batch_size=2\n)\n\n# Clean formatting\nsummaries[\"led\"] = \"\\n\".join(sent_tokenize(\n    led_out[0][\"summary_text\"].replace(\" .<n>\", \". \")\n))\n\n# Print LED summary\nprint(\"=== LED SUMMARY ===\")\nprint(summaries[\"led\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:53:32.502219Z","iopub.execute_input":"2025-09-15T17:53:32.502558Z","iopub.status.idle":"2025-09-15T17:53:54.336261Z","shell.execute_reply.started":"2025-09-15T17:53:32.502527Z","shell.execute_reply":"2025-09-15T17:53:54.335259Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03278d5a8a094901abed0c7ec5eafc7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/648M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f00e9d14a324e629015520cf7b85c8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad1c1c558b724561b26b3110832e25e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf654442c89649a5aace0f2f01467b57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b13d7ff1b58b417db30112b369aeba28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0bdc9559d06466b95ce9a67b5555ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca677e5469840e3bb1f541268593cdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08cb2b6f13c4e8faa073fb659448cb7"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nInput ids are automatically padded from 204 to 1024 to be a multiple of `config.attention_window`: 1024\nBoth `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"=== LED SUMMARY ===\nbroadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord btc 95 uk home business receive broadband phone phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import pipeline\nfrom nltk.tokenize import sent_tokenize\n\n# Load the ProphetNet summarization model\nprophetnet_pipe = pipeline(\"summarization\", model=\"microsoft/prophetnet-large-uncased\", device=0)\n\n# Generate summary with tuned hyperparameters\npipe_output = prophetnet_pipe(\n    sample_text,\n    max_length=150,            # summary length cap\n    min_length=40,             # encourage non-trivial summary\n    num_beams=5,               # beam search\n    no_repeat_ngram_size=3,    # avoid repetition\n    early_stopping=True\n)\n\n# Store the summary in the summaries dictionary\nsummaries[\"prophetnet\"] = \"\\n\".join(sent_tokenize(pipe_output[0][\"summary_text\"].strip()))\n\n# Print the ProphetNet summary\nprint(\"=== ProphetNet SUMMARY ===\")\nprint(summaries[\"prophetnet\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:53:54.337667Z","iopub.execute_input":"2025-09-15T17:53:54.338009Z","iopub.status.idle":"2025-09-15T17:54:48.978597Z","shell.execute_reply.started":"2025-09-15T17:53:54.337986Z","shell.execute_reply":"2025-09-15T17:54:48.977660Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a315996ec06244bb961729ec7c3b3aee"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c4e4529013403d8a49439bcd608adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2dabac366746839c532c3c6c06635d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d334ed9b506a426699d98d439ba0462f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"prophetnet.tokenizer: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"612591eeb59745748b6f4d013c5c3de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f175262766477a816be114a7d33c8a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"=== ProphetNet SUMMARY ===\nit ' s been a long time coming include a lot of people involved in the process of creating internet ' s fastest internet service available anywhere in the world include include us , uk , and canada included include us include canada include us includes canada include uk include us and canada include include uk includes us include us included in the list of people in the united states of america include many company re - sell many companies re - sale many company sell many company buy many companies buy many company get service get service buy company buy company sell company sell much company sell more company sell one company sell two company sell three company sell four company sell six company buy one company buy two company buy - sell company buy more company buy new company sell new company buy another company buy buy\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom nltk.tokenize import sent_tokenize\n\nset_seed(42)\n\n# Load GPT-2 XL for text generation\ngpt2_pipe = pipeline(\"text-generation\", model=\"gpt2-xl\", device=0)\n\n# Append a summarization prompt to the article text\ngpt2_query = sample_text + \"\\nTL;DR:\\n\"\n\n# Generate the output (summary) with tuned hyperparameters\ngpt2_out = gpt2_pipe(\n    gpt2_query,\n    max_new_tokens=150,       # summary length cap\n    num_beams=8,              # beam search for better quality\n    early_stopping=True,      # stop at EOS\n    no_repeat_ngram_size=3,   # avoid repetition\n    batch_size=2,\n    clean_up_tokenization_spaces=True\n)\n\n# Extract summary text after the TL;DR: prompt\nsummaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(\n    gpt2_out[0][\"generated_text\"][len(gpt2_query):].strip()\n))\n\n# Print the GPT-2 summary\nprint(\"=== GPT-2 SUMMARY ===\")\nprint(summaries[\"gpt2\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:54:48.979621Z","iopub.execute_input":"2025-09-15T17:54:48.979887Z","iopub.status.idle":"2025-09-15T17:57:41.657342Z","shell.execute_reply.started":"2025-09-15T17:54:48.979866Z","shell.execute_reply":"2025-09-15T17:57:41.655897Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"=== GPT-2 SUMMARY ===\nThe number of people in the UK who have access to a broadband connection of at least 1Mb/s is 1,7 million.\nThe total number of UK broadband customers is 4.5 million.\nThe number of broadband customers in the rest of the EU is 1.2 million.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"GROUND TRUTH\")\nprint(dataset[\"train\"][1][\"highlights\"])\nprint(\"-\"*100)\nfor model_name in summaries:\n    print(\"##\",model_name.upper(),'\\n')\n    print(summaries[model_name])\n    print(\"-\"*100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:57:41.658869Z","iopub.execute_input":"2025-09-15T17:57:41.659401Z","iopub.status.idle":"2025-09-15T17:57:41.739304Z","shell.execute_reply.started":"2025-09-15T17:57:41.659369Z","shell.execute_reply":"2025-09-15T17:57:41.738102Z"}},"outputs":[{"name":"stdout","text":"GROUND TRUTH\nturkcell 's mobile deal second turkish investment iran run trouble.the company say prepare accept minority stake iran award mobile deal.turkey 's investment iran 's mobile industry look set scrap big mobile firm saw investment slash mps.although company 's statement say would continue monitor development observer say think turkcell set pull 3bn deal.iran 's parliament vote large majority cut turkcell 's stake new mobile network 70 49\n----------------------------------------------------------------------------------------------------\n## BASELINE \n\nbroadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord bt 95 uk home business receive broadband phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk\n----------------------------------------------------------------------------------------------------\n## XLNET \n\nbroadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord bt 95 uk home business receive broadband phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk of a billion pound. Compare your own experience with that of someone else's. You may not be able to meet the same standards as someone else. If you don't have a business in the UK, then you should see how much it costs to get to a competitive market. This means that you should be able to pay the same amount as someone else. That you can afford to pay the same amount as someone else will make it easier for you to get to a competitive market. If you do not have a business in the UK, then you should know how much you can afford to pay. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If\n----------------------------------------------------------------------------------------------------\n## GPT2 \n\nThe number of people in the UK who have access to a broadband connection of at least 1Mb/s is 1,7 million.\nThe total number of UK broadband customers is 4.5 million.\nThe number of broadband customers in the rest of the EU is 1.2 million.\n----------------------------------------------------------------------------------------------------\n## T5 \n\none person uk join internet's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace.\n----------------------------------------------------------------------------------------------------\n## BIGBIRD_PEGASUS \n\nthe number of cable broadband customers in the united kingdom has grown to 1.7 million by end of 2006 , an increase of a factor of three compared to the same period a year ago .<n> cable broadband market penetration in the uk is one of the highest in the world , followed by the arab world .\nin the first quarter of 2007 , the company s revenue and profit increased more than twofold and three times , respectively , over the first three months of the year , as reported by the company in its annual report .\n<n> the rapid increase of the popularity of the world wide web ( ww ) and the information technology ( ht ) has led to an explosion of interest in developing new ways to bring information to people , process , and information more quickly and conveniently than before .\none way to achieve this is the use of asymmetric communication technology ( ait ) , which has become popular in the telecommunications industry during the last decade , see e.g.\n@xcite for a review .\nnowadays ,\n----------------------------------------------------------------------------------------------------\n## BART \n\n broadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million.\nconnect via cable almost six million people fast always-on connection boom fuel fierce competition.\n----------------------------------------------------------------------------------------------------\n## DISTILBART \n\nbroadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people .\nFast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace .\n----------------------------------------------------------------------------------------------------\n## PEGASUS \n\nTelecom giant say number of people broadband via telephone line surpass four million.\nAlmost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt .\n----------------------------------------------------------------------------------------------------\n## LED \n\nbroadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord btc 95 uk home business receive broadband phone phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk\n----------------------------------------------------------------------------------------------------\n## PROPHETNET \n\nit ' s been a long time coming include a lot of people involved in the process of creating internet ' s fastest internet service available anywhere in the world include include us , uk , and canada included include us include canada include us includes canada include uk include us and canada include include uk includes us include us included in the list of people in the united states of america include many company re - sell many companies re - sale many company sell many company buy many companies buy many company get service get service buy company buy company sell company sell much company sell more company sell one company sell two company sell three company sell four company sell six company buy one company buy two company buy - sell company buy more company buy new company sell new company buy another company buy buy\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:57:41.740441Z","iopub.execute_input":"2025-09-15T17:57:41.741119Z","iopub.status.idle":"2025-09-15T17:57:57.367556Z","shell.execute_reply.started":"2025-09-15T17:57:41.741093Z","shell.execute_reply":"2025-09-15T17:57:57.366127Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6a2c1b8b30f2a0001f2a34a0e643ecd801f30d0e874a5d2e72bbf9ad699b9be2\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Evaluation with Rouge metric","metadata":{}},{"cell_type":"code","source":"!pip -q install evaluate bert-score transformers\n\nfrom evaluate import load\nimport pandas as pd\n\n# Use the same test index as used in `sample_text`\nsample_index = 1\nreference = dataset[\"test\"][sample_index][\"highlights\"]\n\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrecords = []\n\nfor model_name in summaries:\n    rouge = load(\"rouge\")\n    prediction = summaries[model_name]\n    rouge.add(prediction=prediction, reference=reference)\n    score = rouge.compute()\n    rouge_dict = {rn: score[rn] for rn in rouge_names}\n    records.append(rouge_dict)\n\ndf = pd.DataFrame.from_records(records, index=summaries.keys())\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:57:57.369665Z","iopub.execute_input":"2025-09-15T17:57:57.370326Z","iopub.status.idle":"2025-09-15T18:00:26.093292Z","shell.execute_reply.started":"2025-09-15T17:57:57.370284Z","shell.execute_reply":"2025-09-15T18:00:26.091747Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2331c0f0175f4694a6438bf671f9c4f2"}},"metadata":{}},{"name":"stdout","text":"                   rouge1    rouge2    rougeL  rougeLsum\nbaseline         0.562249  0.518219  0.305221   0.305221\nxlnet            0.297694  0.269474  0.159329   0.159329\ngpt2             0.241935  0.032787  0.193548   0.129032\nt5               0.473282  0.356589  0.366412   0.366412\nbigbird_pegasus  0.106557  0.033058  0.098361   0.098361\nbart             0.470588  0.358974  0.386555   0.386555\ndistilbart       0.488889  0.345865  0.355556   0.355556\npegasus          0.420168  0.324786  0.336134   0.336134\nled              0.560000  0.508065  0.304000   0.304000\nprophetnet       0.129032  0.037209  0.082949   0.082949\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from evaluate import load\nimport torch, os, pandas as pd\n!pip install bert_score\n\n# metrics\nrouge  = load(\"rouge\")\nbleu   = load(\"bleu\")          # or \"sacrebleu\"\nmeteor = load(\"meteor\")\nbertscore = load(\"bertscore\")  # requires `bert-score` pkg + model weights\n\nBERTSCORE_MODEL = os.environ.get(\"BERTSCORE_MODEL\", \"roberta-base\")\n\ndef compute_bertscore_f1(prediction: str, reference: str) -> float:\n    # Guard against empties\n    if not isinstance(prediction, str) or not prediction.strip():\n        return 0.0\n    if not isinstance(reference, str) or not reference.strip():\n        return 0.0\n    out = bertscore.compute(\n        predictions=[prediction],\n        references=[reference],\n        lang=\"en\",\n        model_type=BERTSCORE_MODEL,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n    return float(out[\"f1\"][0])\n\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrecords = []\n\nsample_index = 1\nreference = dataset[\"test\"][sample_index][\"highlights\"]\n\nfor model_name, prediction in summaries.items():\n    r = rouge.compute(predictions=[prediction], references=[reference], use_stemmer=True)\n    b = bleu.compute(predictions=[prediction], references=[[reference]])\n    m = meteor.compute(predictions=[prediction], references=[reference])\n    try:\n        bs_f1 = compute_bertscore_f1(prediction, reference)\n    except Exception as e:\n        print(f\"[BERTScore error for {model_name}]:\", e)\n        bs_f1 = None\n\n    row = {rn: r[rn] for rn in rouge_names}\n    row.update({\"bleu\": b.get(\"bleu\", b.get(\"score\")), \"meteor\": m[\"meteor\"], \"bertscore_f1\": bs_f1})\n    records.append(row)\n\n    print(f\"=== {model_name.upper()} ===\")\n    print(\"Generated Summary:\", prediction)\n    print(\"Reference Summary:\", reference)\n    print(\"BLEU:\", row[\"bleu\"], \" METEOR:\", row[\"meteor\"], \" BERTScore(F1):\", row[\"bertscore_f1\"])\n    print(\"-\"*100)\n\ndf = pd.DataFrame.from_records(records, index=summaries.keys())\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:00:26.095360Z","iopub.execute_input":"2025-09-15T18:00:26.095793Z","iopub.status.idle":"2025-09-15T18:00:54.832752Z","shell.execute_reply.started":"2025-09-15T18:00:26.095755Z","shell.execute_reply":"2025-09-15T18:00:54.830645Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.52.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.4)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.33.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.6.15)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b82e8f64c641909add1571a59622e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522d4d87c6dc4cbda38276ad2c223191"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e2f36c68b14350a3966f920a5db92b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb2a9e37fa143e4bc30a380d16e9159"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae1ff713ec634be097dd5b443d23ae8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6692c5193442dbad0f8d9b69a533f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e93df201cac45119f9e90518f88681b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e6d675123fb484cbac9073ae4f01f60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"787d1f5f677f4f0abcb41c6e1c24f3d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14b053b7c81548ac9a6f10b155819b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b141257932e44ae8da854ca195e8c5d"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"=== BASELINE ===\nGenerated Summary: broadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord bt 95 uk home business receive broadband phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.34394239843199864  METEOR: 0.6595838747418761  BERTScore(F1): 0.8661757111549377\n----------------------------------------------------------------------------------------------------\n=== XLNET ===\nGenerated Summary: broadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord bt 95 uk home business receive broadband phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk of a billion pound. Compare your own experience with that of someone else's. You may not be able to meet the same standards as someone else. If you don't have a business in the UK, then you should see how much it costs to get to a competitive market. This means that you should be able to pay the same amount as someone else. That you can afford to pay the same amount as someone else will make it easier for you to get to a competitive market. If you do not have a business in the UK, then you should know how much you can afford to pay. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If you do not have a business in the UK, then you should know what to pay for a product or service that you can afford to pay for. If\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.14338090441907075  METEOR: 0.46819060365053206  BERTScore(F1): 0.809716522693634\n----------------------------------------------------------------------------------------------------\n=== GPT2 ===\nGenerated Summary: The number of people in the UK who have access to a broadband connection of at least 1Mb/s is 1,7 million.\nThe total number of UK broadband customers is 4.5 million.\nThe number of broadband customers in the rest of the EU is 1.2 million.\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.0  METEOR: 0.0998463901689708  BERTScore(F1): 0.8316341638565063\n----------------------------------------------------------------------------------------------------\n=== T5 ===\nGenerated Summary: one person uk join internet's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace.\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.26553784290661075  METEOR: 0.3883823032759203  BERTScore(F1): 0.8812610507011414\n----------------------------------------------------------------------------------------------------\n=== BIGBIRD_PEGASUS ===\nGenerated Summary: the number of cable broadband customers in the united kingdom has grown to 1.7 million by end of 2006 , an increase of a factor of three compared to the same period a year ago .<n> cable broadband market penetration in the uk is one of the highest in the world , followed by the arab world .\nin the first quarter of 2007 , the company s revenue and profit increased more than twofold and three times , respectively , over the first three months of the year , as reported by the company in its annual report .\n<n> the rapid increase of the popularity of the world wide web ( ww ) and the information technology ( ht ) has led to an explosion of interest in developing new ways to bring information to people , process , and information more quickly and conveniently than before .\none way to achieve this is the use of asymmetric communication technology ( ait ) , which has become popular in the telecommunications industry during the last decade , see e.g.\n@xcite for a review .\nnowadays ,\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.0  METEOR: 0.13986089791953935  BERTScore(F1): 0.7997866272926331\n----------------------------------------------------------------------------------------------------\n=== BART ===\nGenerated Summary:  broadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million.\nconnect via cable almost six million people fast always-on connection boom fuel fierce competition.\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.2378359977805848  METEOR: 0.33466652056395646  BERTScore(F1): 0.8883223533630371\n----------------------------------------------------------------------------------------------------\n=== DISTILBART ===\nGenerated Summary: broadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people .\nFast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace .\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.24975628790024082  METEOR: 0.39489565136868804  BERTScore(F1): 0.8768050670623779\n----------------------------------------------------------------------------------------------------\n=== PEGASUS ===\nGenerated Summary: Telecom giant say number of people broadband via telephone line surpass four million.\nAlmost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt .\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.16434332711829372  METEOR: 0.2789181907951228  BERTScore(F1): 0.8696805834770203\n----------------------------------------------------------------------------------------------------\n=== LED ===\nGenerated Summary: broadband uk gather pace one person uk join internet 's fast lane every 10 second accord bt telecom giant say number people broadband via telephone line surpass four million include connect via cable almost six million people fast always-on connection boom fuel fierce competition fall price well great availability broadband phone line take-up rate broadband accelerate terrific pace say ben verwaayen bt 's chief executive strong position hit five million target summer 2006 much early previously expect last million connection make past four month thousand people add total every day week sign broadband include get service direct bt via many company re-sell bt line name part surge people sign due bt stretch reach adsl uk 's widely used way get broadband beyond six kilometre asymmetric digital subscriber line technology let ordinary copper phone line support high data speed standard speed 512kbps though fast connection available accord btc 95 uk home business receive broadband phone phone line aim extend figure 99.4 next summer also estimate 1.7 million cable broadband customer uk\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.33542009143623164  METEOR: 0.6587305967667897  BERTScore(F1): 0.8654277920722961\n----------------------------------------------------------------------------------------------------\n=== PROPHETNET ===\nGenerated Summary: it ' s been a long time coming include a lot of people involved in the process of creating internet ' s fastest internet service available anywhere in the world include include us , uk , and canada included include us include canada include us includes canada include uk include us and canada include include uk includes us include us included in the list of people in the united states of america include many company re - sell many companies re - sale many company sell many company buy many companies buy many company get service get service buy company buy company sell company sell much company sell more company sell one company sell two company sell three company sell four company sell six company buy one company buy two company buy - sell company buy more company buy new company sell new company buy another company buy buy\nReference Summary: telecom giant say number people broadband via telephone line surpass four million.according bt 95 uk home business receive broadband phone line.there also estimate 1.7 million cable broadband customer uk.those sign broadband include get service direct bt via many company re-sell bt line name.including connect via cable almost six million people fast always-on connection.the last million connection make past four month thousand people add total every day week\nBLEU: 0.0  METEOR: 0.11838061465721042  BERTScore(F1): 0.7726821303367615\n----------------------------------------------------------------------------------------------------\n                   rouge1    rouge2    rougeL  rougeLsum      bleu    meteor  \\\nbaseline         0.578313  0.534413  0.305221   0.305221  0.343942  0.659584   \nxlnet            0.306080  0.277895  0.159329   0.159329  0.143381  0.468191   \ngpt2             0.258065  0.049180  0.193548   0.161290  0.000000  0.099846   \nt5               0.488550  0.387597  0.366412   0.366412  0.265538  0.388382   \nbigbird_pegasus  0.122951  0.041322  0.098361   0.106557  0.000000  0.139861   \nbart             0.487395  0.393162  0.386555   0.386555  0.237836  0.334667   \ndistilbart       0.503704  0.375940  0.355556   0.355556  0.249756  0.394896   \npegasus          0.420168  0.324786  0.336134   0.336134  0.164343  0.278918   \nled              0.576000  0.524194  0.304000   0.304000  0.335420  0.658731   \nprophetnet       0.138249  0.037209  0.082949   0.082949  0.000000  0.118381   \n\n                 bertscore_f1  \nbaseline             0.866176  \nxlnet                0.809717  \ngpt2                 0.831634  \nt5                   0.881261  \nbigbird_pegasus      0.799787  \nbart                 0.888322  \ndistilbart           0.876805  \npegasus              0.869681  \nled                  0.865428  \nprophetnet           0.772682  \n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# # --- Install deps (safe to re-run) ---\n# !pip -q install --upgrade evaluate bert-score moverscore transformers\n\n# import os, sys, traceback, torch\n# import pandas as pd\n# import numpy as np\n# from evaluate import load\n\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# # ---------- Ensure dataset exists (auto-load if missing) ----------\n# try:\n#     dataset  # noqa: F821\n# except NameError:\n#     print(\"[INFO] 'dataset' missing — loading CSV and creating splits...\")\n#     import pandas as _pd\n#     from datasets import Dataset, DatasetDict\n#     CSV_PATH = \"/kaggle/input/reviews-summary-sentiment/Sentiment_summary_reviews.csv\"\n#     df = _pd.read_csv(CSV_PATH)\n#     df = df[['Text', 'Summary']].rename(columns={'Text':'article','Summary':'highlights'})\n#     df.dropna(subset=[\"article\",\"highlights\"], inplace=True)\n#     df = df[df['article'].str.len()>30]\n#     df = df[df['highlights'].str.len()>10]\n#     def _clean(s): return \" \".join(str(s).strip().split())\n#     df[\"article\"] = df[\"article\"].map(_clean)\n#     df[\"highlights\"] = df[\"highlights\"].map(_clean)\n#     from datasets import Dataset, DatasetDict\n#     hf_all = Dataset.from_pandas(df.reset_index(drop=True))\n#     tmp = hf_all.train_test_split(test_size=0.2, seed=42)\n#     tv = tmp[\"train\"].train_test_split(test_size=0.125, seed=42)  # 10% val\n#     dataset = DatasetDict({\"train\": tv[\"train\"], \"validation\": tv[\"test\"], \"test\": tmp[\"test\"]})\n#     print(\"[INFO] dataset ready:\", dataset)\n\n# # ---------- Ensure summaries exists ----------\n# try:\n#     summaries  # noqa: F821\n# except NameError:\n#     raise RuntimeError(\"Define `summaries` like {'baseline':'...', 'gpt2':'...', ...} for the SAME sample_index.\")\n\n# # ---------- Config ----------\n# sample_index = 1\n# reference = dataset[\"test\"][sample_index][\"highlights\"]\n\n# # ---------- Metrics ----------\n# rouge = load(\"rouge\")\n\n# try:\n#     sacrebleu = load(\"sacrebleu\")\n#     use_sacre_bleu = True\n# except Exception:\n#     sacrebleu = None\n#     use_sacre_bleu = False\n#     bleu = load(\"bleu\")\n\n# meteor = load(\"meteor\")\n\n# # BERTScore\n# try:\n#     bertscore = load(\"bertscore\")\n#     BERTSCORE_MODEL = os.environ.get(\"BERTSCORE_MODEL\", \"roberta-base\")  # 'roberta-large' if you want\n#     has_bertscore = True\n# except Exception as e:\n#     print(\"[WARN] BERTScore unavailable ->\", e); bertscore=None; has_bertscore=False\n\n# # ---------- MoverScore (patched) ----------\n# try:\n#     import moverscore_v2 as ms\n#     from transformers import AutoTokenizer\n\n#     # Choose a stable encoder (works well with moverscore_v2)\n#     MOVER_MODEL = os.environ.get(\"MOVER_MODEL\", \"bert-base-uncased\")\n\n#     # Patch the module's global tokenizer so it has `max_len`\n#     ms.tokenizer = AutoTokenizer.from_pretrained(MOVER_MODEL, use_fast=False)\n#     if not hasattr(ms.tokenizer, \"max_len\"):\n#         # For old moverscore code that expects `max_len`\n#         ms.tokenizer.max_len = getattr(ms.tokenizer, \"model_max_length\", 512)\n\n#     from moverscore_v2 import get_idf_dict, word_mover_score\n#     has_moverscore = True\n# except Exception as e:\n#     print(\"[WARN] MoverScore unavailable ->\", e)\n#     ms = None; get_idf_dict=None; word_mover_score=None; has_moverscore=False\n\n# def compute_bleu(pred, ref):\n#     if use_sacre_bleu:\n#         return float(sacrebleu.compute(predictions=[pred], references=[[ref]]).get(\"score\",0.0))\n#     else:\n#         return float(bleu.compute(predictions=[pred], references=[ref]).get(\"bleu\",0.0))\n\n# def compute_bertscore_f1(pred, ref):\n#     if not has_bertscore: return None\n#     try:\n#         out = bertscore.compute(predictions=[pred], references=[ref],\n#                                 lang=\"en\", model_type=BERTSCORE_MODEL, device=DEVICE)\n#         return float(out[\"f1\"][0])\n#     except Exception as e:\n#         print(\"[BERTScore error]\", e); return None\n\n# # Precompute reference IDF once; set nthreads=1 to avoid fork/pool issues\n# _idf_ref = None\n# if has_moverscore:\n#     try:\n#         _idf_ref = get_idf_dict([reference], nthreads=1)\n#         print(f\"[MoverScore] Using {MOVER_MODEL} on {DEVICE}\")\n#     except Exception as e:\n#         print(\"[MoverScore init error]\", e); traceback.print_exc(); _idf_ref=None\n\n# def compute_moverscore(pred, ref):\n#     if not has_moverscore or _idf_ref is None: return None\n#     if not isinstance(pred, str) or not pred.strip(): return 0.0\n#     try:\n#         idf_hyp = get_idf_dict([pred], nthreads=1)\n#         scores = word_mover_score(\n#             [ref], [pred],\n#             _idf_ref, idf_hyp,\n#             stop_words=[], n_gram=1, remove_subwords=True,\n#             batch_size=8, model_name=MOVER_MODEL, device=DEVICE\n#         )\n#         return float(scores[0])\n#     except Exception as e:\n#         print(\"[MoverScore error]\", e)\n#         # CPU fallback if GPU tokenizer/model causes issues\n#         if DEVICE == \"cuda\":\n#             try:\n#                 scores = word_mover_score(\n#                     [ref], [pred],\n#                     _idf_ref, get_idf_dict([pred], nthreads=1),\n#                     stop_words=[], n_gram=1, remove_subwords=True,\n#                     batch_size=8, model_name=MOVER_MODEL, device=\"cpu\"\n#                 )\n#                 print(\"[MoverScore] CPU fallback OK\")\n#                 return float(scores[0])\n#             except Exception as e2:\n#                 print(\"[MoverScore CPU fallback error]\", e2)\n#         return None\n\n# # ---------- Evaluate all models ----------\n# rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n# records = []\n\n# print(\"-\"*100)\n# for model_name, prediction in summaries.items():\n#     r = rouge.compute(predictions=[prediction], references=[reference], use_stemmer=True)\n#     bleu_score = compute_bleu(prediction, reference)\n#     m = meteor.compute(predictions=[prediction], references=[reference])\n#     bert_f1 = compute_bertscore_f1(prediction, reference)\n#     mover = compute_moverscore(prediction, reference)\n\n#     row = {rn: r[rn] for rn in rouge_names}\n#     row.update({\"bleu\": bleu_score, \"meteor\": m[\"meteor\"], \"bertscore_f1\": bert_f1, \"moverscore\": mover})\n#     records.append(row)\n\n#     print(f\"=== {model_name.upper()} ===\")\n#     print(\"ROUGE-1:\", row[\"rouge1\"], \"| ROUGE-2:\", row[\"rouge2\"],\n#           \"| ROUGE-L:\", row[\"rougeL\"], \"| ROUGE-Lsum:\", row[\"rougeLsum\"])\n#     print(\"BLEU:\", row[\"bleu\"], \"| METEOR:\", row[\"meteor\"],\n#           \"| BERTScore(F1):\", row[\"bertscore_f1\"], \"| MoverScore:\", row[\"moverscore\"])\n#     print(\"-\"*100)\n\n# df = pd.DataFrame.from_records(records, index=list(summaries.keys()))\n# print(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:00:54.834589Z","iopub.execute_input":"2025-09-15T18:00:54.834929Z","iopub.status.idle":"2025-09-15T18:00:54.843996Z","shell.execute_reply.started":"2025-09-15T18:00:54.834899Z","shell.execute_reply":"2025-09-15T18:00:54.842835Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# # --- Install deps (safe to re-run) ---\n# !pip -q install --upgrade evaluate bert-score moverscore transformers\n\n# import os, sys, traceback, torch\n# import pandas as pd\n# import numpy as np\n# from evaluate import load\n\n# # Silence parallelism warning\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# # ---------- Ensure dataset exists (auto-load if missing) ----------\n# try:\n#     dataset  # noqa: F821\n# except NameError:\n#     print(\"[INFO] 'dataset' missing — loading CSV and creating splits...\")\n#     import pandas as _pd\n#     from datasets import Dataset, DatasetDict\n#     CSV_PATH = \"/kaggle/input/cnn-dataser/cnn_dailymail_summary.csv\"\n#     df = _pd.read_csv(CSV_PATH)\n#     df = df[['Text', 'Summary']].rename(columns={'Text':'article','Summary':'highlights'})\n#     df.dropna(subset=[\"article\",\"highlights\"], inplace=True)\n#     df = df[df['article'].str.len()>30]\n#     df = df[df['highlights'].str.len()>10]\n#     def _clean(s): return \" \".join(str(s).strip().split())\n#     df[\"article\"] = df[\"article\"].map(_clean)\n#     df[\"highlights\"] = df[\"highlights\"].map(_clean)\n#     from datasets import Dataset, DatasetDict\n#     hf_all = Dataset.from_pandas(df.reset_index(drop=True))\n#     tmp = hf_all.train_test_split(test_size=0.2, seed=42)\n#     tv = tmp[\"train\"].train_test_split(test_size=0.125, seed=42)  # 10% val\n#     dataset = DatasetDict({\"train\": tv[\"train\"], \"validation\": tv[\"test\"], \"test\": tmp[\"test\"]})\n#     print(\"[INFO] dataset ready:\", dataset)\n\n# # ---------- Ensure summaries exists ----------\n# try:\n#     summaries  # noqa: F821\n# except NameError:\n#     raise RuntimeError(\"Define `summaries` like {'baseline':'...', 'gpt2':'...', ...} for the SAME sample_index.\")\n\n# # ---------- Config ----------\n# sample_index = 1\n# reference = dataset[\"test\"][sample_index][\"highlights\"]\n\n# # ---------- Metrics ----------\n# rouge = load(\"rouge\")\n# try:\n#     sacrebleu = load(\"sacrebleu\"); use_sacre_bleu = True\n# except Exception:\n#     sacrebleu = None; use_sacre_bleu = False; bleu = load(\"bleu\")\n# meteor = load(\"meteor\")\n\n# # BERTScore (GPU ok)\n# try:\n#     bertscore = load(\"bertscore\")\n#     BERTSCORE_MODEL = os.environ.get(\"BERTSCORE_MODEL\", \"roberta-base\")  # 'roberta-large' if needed\n#     has_bertscore = True\n# except Exception as e:\n#     print(\"[WARN] BERTScore unavailable ->\", e); bertscore=None; has_bertscore=False\n\n# def compute_bleu(pred, ref):\n#     if use_sacre_bleu:\n#         return float(sacrebleu.compute(predictions=[pred], references=[[ref]]).get(\"score\",0.0))\n#     else:\n#         return float(bleu.compute(predictions=[pred], references=[ref]).get(\"bleu\",0.0))\n\n# def compute_bertscore_f1(pred, ref):\n#     if not has_bertscore: return None\n#     try:\n#         out = bertscore.compute(predictions=[pred], references=[ref],\n#                                 lang=\"en\", model_type=BERTSCORE_MODEL,\n#                                 device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#         return float(out[\"f1\"][0])\n#     except Exception as e:\n#         print(\"[BERTScore error]\", e); return None\n\n# # ---------- MoverScore (v1.0.3-safe, CPU-only) ----------\n# # IMPORTANT: do not pass 'model_name' or 'device' to word_mover_score in this version.\n# try:\n#     import moverscore_v2 as ms\n#     from transformers import AutoTokenizer\n#     from moverscore_v2 import get_idf_dict, word_mover_score\n\n#     # Use a stable encoder and patch tokenizer.max_len expected by old code\n#     MOVER_MODEL = os.environ.get(\"MOVER_MODEL\", \"bert-base-uncased\")\n#     ms.tokenizer = AutoTokenizer.from_pretrained(MOVER_MODEL, use_fast=False)\n#     if not hasattr(ms.tokenizer, \"max_len\"):\n#         ms.tokenizer.max_len = getattr(ms.tokenizer, \"model_max_length\", 512)\n\n#     has_moverscore = True\n# except Exception as e:\n#     print(\"[WARN] MoverScore unavailable ->\", e)\n#     has_moverscore = False\n#     get_idf_dict = None\n#     word_mover_score = None\n\n# # Build IDF for the single reference (nthreads=1 to avoid multiprocessing/fork issues)\n# _idf_ref = None\n# if has_moverscore:\n#     try:\n#         _idf_ref = get_idf_dict([reference], nthreads=1)  # v1.0.3 signature\n#         print(\"[MoverScore] Ready (CPU-only) with model:\", MOVER_MODEL)\n#     except Exception as e:\n#         print(\"[MoverScore init error]\", e); traceback.print_exc(); _idf_ref=None\n\n# def compute_moverscore(pred, ref):\n#     \"\"\"MoverScore for one pair using v1.0.3; CPU-only; no model_name/device kwargs.\"\"\"\n#     if not has_moverscore or _idf_ref is None:\n#         return None\n#     pred = (pred or \"\").strip()\n#     ref  = (ref  or \"\").strip()\n#     if not pred or not ref:\n#         return 0.0\n#     try:\n#         idf_hyp = get_idf_dict([pred], nthreads=1)\n#         # DO NOT pass device or model_name here (not supported in 1.0.3)\n#         scores = word_mover_score(\n#             [ref], [pred],\n#             _idf_ref, idf_hyp,\n#             stop_words=[], n_gram=1, remove_subwords=True,\n#             batch_size=8\n#         )\n#         return float(scores[0])\n#     except Exception as e:\n#         print(\"[MoverScore error]\", e)\n#         return None\n\n# # ---------- Evaluate all models ----------\n# rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n# records = []\n\n# print(\"-\"*100)\n# for model_name, prediction in summaries.items():\n#     r = rouge.compute(predictions=[prediction], references=[reference], use_stemmer=True)\n#     bleu_score = compute_bleu(prediction, reference)\n#     m = meteor.compute(predictions=[prediction], references=[reference])\n#     bert_f1 = compute_bertscore_f1(prediction, reference)\n#     mover = compute_moverscore(prediction, reference)\n\n#     row = {rn: r[rn] for rn in rouge_names}\n#     row.update({\"bleu\": bleu_score, \"meteor\": m[\"meteor\"], \"bertscore_f1\": bert_f1, \"moverscore\": mover})\n#     records.append(row)\n\n#     print(f\"=== {model_name.upper()} ===\")\n#     print(\"ROUGE-1:\", row[\"rouge1\"], \"| ROUGE-2:\", row[\"rouge2\"],\n#           \"| ROUGE-L:\", row[\"rougeL\"], \"| ROUGE-Lsum:\", row[\"rougeLsum\"])\n#     print(\"BLEU:\", row[\"bleu\"], \"| METEOR:\", row[\"meteor\"],\n#           \"| BERTScore(F1):\", row[\"bertscore_f1\"], \"| MoverScore:\", row[\"moverscore\"])\n#     print(\"-\"*100)\n\n# df = pd.DataFrame.from_records(records, index=list(summaries.keys()))\n# print(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:00:54.845242Z","iopub.execute_input":"2025-09-15T18:00:54.845569Z","iopub.status.idle":"2025-09-15T18:00:54.878697Z","shell.execute_reply.started":"2025-09-15T18:00:54.845532Z","shell.execute_reply":"2025-09-15T18:00:54.877292Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# --- Use the GitHub version of MoverScore + robust wrapper ---\n# (Safe to re-run. If you're offline, it will keep the existing install.)\n\ntry:\n    import subprocess, sys\n    # Uninstall old PyPI build that causes crashes\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"moverscore\"], check=False)\n    # Install latest master from GitHub (has sentence_score / corpus_score)\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/AIPHES/emnlp19-moverscore.git@master\"], check=False)\nexcept Exception as _e:\n    print(\"[WARN] Could not (re)install moverscore from GitHub:\", _e)\n\n# --- Robust MoverScore wrapper (prefers sentence_score; no GPU complications) ---\nimport os, traceback\nos.environ.setdefault(\"MOVERSCORE_MODEL\", \"bert-base-uncased\")  # you can switch to e.g. 'albert-base-v2'\n\ndef compute_moverscore(prediction: str, reference: str):\n    \"\"\"\n    Prefer moverscore_v2.sentence_score (stable). If unavailable, return None.\n    Returns a float (higher is better) or None on failure.\n    \"\"\"\n    try:\n        from moverscore_v2 import sentence_score\n        # sentence_score supports multi-refs; we pass a single reference\n        sc = sentence_score(prediction, [reference])\n        # Some versions return a dataclass with `.score`, others a float; handle both.\n        return float(getattr(sc, \"score\", sc))\n    except Exception as e:\n        print(\"[MoverScore sentence_score error]\", e)\n        traceback.print_exc(limit=1)\n        return None\n\n# --- (Optional) quick sanity check ---\nprint(\"MoverScore sanity:\", compute_moverscore(\"A fast brown fox leaps over a lazy dog.\",\n                                               \"The quick brown fox jumps over the lazy dog.\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:00:54.880103Z","iopub.execute_input":"2025-09-15T18:00:54.880467Z","iopub.status.idle":"2025-09-15T18:01:12.946565Z","shell.execute_reply.started":"2025-09-15T18:00:54.880437Z","shell.execute_reply":"2025-09-15T18:01:12.945450Z"}},"outputs":[{"name":"stderr","text":"WARNING: Skipping moverscore as it is not installed.\n","output_type":"stream"},{"name":"stdout","text":"     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 2.1 MB/s eta 0:00:00\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16cdf7b5b4944d5190375e0fa6dcb941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e508eb9dd50541f1a8b83bda7cde7c42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61cfae460eaf4084af2cb9c2b316ee1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"615f6f95a9224736bc46d8660bc6ce6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfba8953bd20454fb4ca7da846ddf60a"}},"metadata":{}},{"name":"stdout","text":"[MoverScore sentence_score error] Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\nMoverScore sanity: None\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_36/1894159147.py\", line 23, in compute_moverscore\n    from moverscore_v2 import sentence_score\nRuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# recompute the full table (ROUGE/BLEU/METEOR/BERTScore/MoverScore)\nfrom evaluate import load\nimport pandas as pd\nimport torch, os\n\nsample_index = 1\nreference = dataset[\"test\"][sample_index][\"highlights\"]\n\nrouge = load(\"rouge\")\ntry:\n    sacrebleu = load(\"sacrebleu\"); use_sacre_bleu = True\nexcept Exception:\n    sacrebleu = None; use_sacre_bleu = False; bleu = load(\"bleu\")\nmeteor = load(\"meteor\")\n\ntry:\n    bertscore = load(\"bertscore\"); BERTSCORE_MODEL = os.environ.get(\"BERTSCORE_MODEL\", \"roberta-base\")\n    has_bertscore = True\nexcept Exception:\n    has_bertscore = False\n\ndef compute_bleu(pred, ref):\n    return (sacrebleu.compute(predictions=[pred], references=[[ref]])[\"score\"]\n            if use_sacre_bleu else\n            load(\"bleu\").compute(predictions=[pred], references=[ref])[\"bleu\"])\n\ndef compute_bertscore_f1(pred, ref):\n    if not has_bertscore: return None\n    out = bertscore.compute(predictions=[pred], references=[ref],\n                            lang=\"en\",\n                            model_type=BERTSCORE_MODEL,\n                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    return float(out[\"f1\"][0])\n\nrows = []\nfor name, pred in summaries.items():\n    r = rouge.compute(predictions=[pred], references=[reference], use_stemmer=True)\n    b = compute_bleu(pred, reference)\n    m = meteor.compute(predictions=[pred], references=[reference])[\"meteor\"]\n    bs = compute_bertscore_f1(pred, reference)\n   # mv = compute_moverscore(pred, reference)   # <-- uses the robust fallback you installed\n\n    rows.append({\n        \"model\": name,\n        \"rouge1\": r[\"rouge1\"], \"rouge2\": r[\"rouge2\"], \"rougeL\": r[\"rougeL\"], \"rougeLsum\": r[\"rougeLsum\"],\n        \"bleu\": b, \"meteor\": m, \"bertscore_f1\": bs})\n\ndf = pd.DataFrame(rows).set_index(\"model\")\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:01:12.950585Z","iopub.execute_input":"2025-09-15T18:01:12.950905Z","iopub.status.idle":"2025-09-15T18:01:28.143919Z","shell.execute_reply.started":"2025-09-15T18:01:12.950873Z","shell.execute_reply":"2025-09-15T18:01:28.142582Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0721b109574d49d1bc41e35977ea7b2c"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"                   rouge1    rouge2    rougeL  rougeLsum      bleu    meteor  \\\nmodel                                                                          \nbaseline         0.578313  0.534413  0.305221   0.305221  0.343942  0.659584   \nxlnet            0.306080  0.277895  0.159329   0.159329  0.143381  0.468191   \ngpt2             0.258065  0.049180  0.193548   0.161290  0.000000  0.099846   \nt5               0.488550  0.387597  0.366412   0.366412  0.265538  0.388382   \nbigbird_pegasus  0.122951  0.041322  0.098361   0.106557  0.000000  0.139861   \nbart             0.487395  0.393162  0.386555   0.386555  0.237836  0.334667   \ndistilbart       0.503704  0.375940  0.355556   0.355556  0.249756  0.394896   \npegasus          0.420168  0.324786  0.336134   0.336134  0.164343  0.278918   \nled              0.576000  0.524194  0.304000   0.304000  0.335420  0.658731   \nprophetnet       0.138249  0.037209  0.082949   0.082949  0.000000  0.118381   \n\n                 bertscore_f1  \nmodel                          \nbaseline             0.866176  \nxlnet                0.809717  \ngpt2                 0.831634  \nt5                   0.881261  \nbigbird_pegasus      0.799787  \nbart                 0.888322  \ndistilbart           0.876805  \npegasus              0.869681  \nled                  0.865428  \nprophetnet           0.772682  \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ===============================\n# Fixed evaluation (no BERTScore): ROUGE, BLEU/SacreBLEU, METEOR, MoverScore\n# ===============================\n# Works with: Hugging Face `evaluate`, `transformers`, and `pot` (optimal transport)\n# Assumes: `dataset` (HF DatasetDict with a \"test\" split) and `summaries` dict already exist.\n\n# ----- Installs (safe to re-run) -----\n!pip -q install --upgrade evaluate transformers pot\n\n# ----- Imports & setup -----\nimport os, sys, traceback, math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\n\nfrom evaluate import load\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ----- Check dataset & summaries availability -----\ntry:\n    dataset  # noqa: F821\nexcept NameError:\n    # Fallback: load your CSV if dataset isn't in memory (adjust path as needed)\n    print(\"[INFO] 'dataset' not found; loading from CSV fallback...\")\n    import pandas as _pd\n    from datasets import Dataset, DatasetDict\n    CSV_PATH = \"/kaggle/input/bbc-news-dataset/bbc_news_summary.csv\"\n    df = _pd.read_csv(CSV_PATH)\n    df = df[['Text', 'Summary']].rename(columns={'Text':'article','Summary':'highlights'})\n    df.dropna(subset=[\"article\",\"highlights\"], inplace=True)\n    df = df[df['article'].str.len()>30]\n    df = df[df['highlights'].str.len()>10]\n    def _clean(s): return \" \".join(str(s).strip().split())\n    df[\"article\"] = df[\"article\"].map(_clean)\n    df[\"highlights\"] = df[\"highlights\"].map(_clean)\n    hf_all = Dataset.from_pandas(df.reset_index(drop=True))\n    tmp = hf_all.train_test_split(test_size=0.2, seed=42)\n    tv  = tmp[\"train\"].train_test_split(test_size=0.125, seed=42)  # 10% val\n    dataset = DatasetDict({\"train\": tv[\"train\"], \"validation\": tv[\"test\"], \"test\": tmp[\"test\"]})\n    print(\"[INFO] dataset ready:\", dataset)\n\ntry:\n    summaries  # noqa: F821\nexcept NameError:\n    raise RuntimeError(\"Please define `summaries` as a dict: {'model_name': 'predicted summary text', ...}\")\n\n# ----- Config -----\nsample_index = 1  # <-- set to the same index you used to generate `summaries`\nreference = dataset[\"test\"][sample_index][\"highlights\"]\n\n# ===============================\n# Robust MoverScore (no moverscore_v2)\n# ===============================\n# Uses sentence-level token embeddings from a transformer encoder and computes\n# an Earth Mover's Distance under cosine-cost with IDF-like weights.\n# Default encoder is 'roberta-large' for stronger scores; auto-falls back to 'roberta-base' if OOM.\n\nfrom transformers import AutoTokenizer, AutoModel\nimport ot  # POT (Python Optimal Transport)\n\ndef _load_encoder(pref: str = \"roberta-large\"):\n    try:\n        tok = AutoTokenizer.from_pretrained(pref)\n        mdl = AutoModel.from_pretrained(pref).eval().to(DEVICE)\n        return tok, mdl, pref\n    except Exception as e:\n        print(f\"[WARN] Failed to load {pref} ({e}); falling back to roberta-base.\")\n        tok = AutoTokenizer.from_pretrained(\"roberta-base\")\n        mdl = AutoModel.from_pretrained(\"roberta-base\").eval().to(DEVICE)\n        return tok, mdl, \"roberta-base\"\n\n_tok, _enc_model, _enc_name = _load_encoder(os.environ.get(\"FALLBACK_MOVER_MODEL\", \"roberta-large\"))\nprint(f\"[MoverScore encoder] {_enc_name} on {DEVICE}\")\n\n_ref_cache = {}  # cache (embeddings, weights) per reference text\n\ndef _tokens_and_embs(text: str):\n    if not isinstance(text, str) or not text.strip():\n        return None, None\n    enc_full = _tok(text,\n                    return_tensors=\"pt\",\n                    return_special_tokens_mask=True,\n                    truncation=True,\n                    max_length=512)\n    input_ids     = enc_full[\"input_ids\"].to(DEVICE)\n    attention_mask= enc_full[\"attention_mask\"].to(DEVICE)\n    with torch.no_grad():\n        out = _enc_model(input_ids=input_ids, attention_mask=attention_mask)\n        hs  = out.last_hidden_state.squeeze(0)  # [seq, hidden] on DEVICE\n\n    # Build masks on CPU, then move the boolean indexer to hs.device\n    special = enc_full[\"special_tokens_mask\"].squeeze(0).bool()   # CPU\n    mask    = enc_full[\"attention_mask\"].squeeze(0).bool()        # CPU\n    keep    = mask & (~special)                                   # CPU\n    keep_dev= keep.to(hs.device)\n\n    hs = hs[keep_dev]  # filter special tokens\n    if hs.numel() == 0:\n        return None, None\n\n    # Normalize so cosine = dot product\n    hs = F.normalize(hs, p=2, dim=1)\n\n    toks = _tok.convert_ids_to_tokens(enc_full[\"input_ids\"].squeeze(0)[keep].tolist())\n    return toks, hs\n\ndef _idf_like_weights(tokens):\n    # simple per-sentence IDF surrogate: 1/(1+tf)\n    from collections import Counter\n    c = Counter(tokens)\n    w = torch.tensor([1.0/(1.0+c[t]) for t in tokens], dtype=torch.float32, device=DEVICE)\n    w = w / (w.sum() + 1e-12)\n    return w\n\ndef _emd_cosine_score(E_ref, E_hyp, w_ref, w_hyp):\n    # cost = 1 - cosine(emb_i, emb_j)  (embeddings are normalized)\n    C = 1.0 - torch.mm(E_ref, E_hyp.t())                           # [Nr, Nh] on DEVICE\n    C = C.detach().cpu().numpy().astype(np.float64)\n    a = w_ref.detach().cpu().numpy().astype(np.float64)\n    b = w_hyp.detach().cpu().numpy().astype(np.float64)\n    # EMD (Wasserstein-1 squared) via POT\n    emd = ot.emd2(a, b, C)                                         # scalar\n    return float(1.0 / (1.0 + emd))                                 # map distance -> [0,1]\n\ndef compute_moverscore(prediction: str, reference: str) -> float | None:\n    \"\"\"Stable MoverScore using transformer embeddings + OT; returns float or None.\"\"\"\n    pred = (prediction or \"\").strip()\n    ref  = (reference or \"\").strip()\n    if not pred or not ref:\n        return None\n\n    # cache reference once\n    if ref not in _ref_cache:\n        t_r, E_r = _tokens_and_embs(ref)\n        if E_r is None:\n            return None\n        w_r = _idf_like_weights(t_r)\n        _ref_cache[ref] = (E_r.detach(), w_r.detach())  # keep on DEVICE\n\n    E_r, w_r = _ref_cache[ref]\n    # Ensure cache tensors are on current DEVICE\n    E_r = E_r.to(DEVICE)\n    w_r = w_r.to(DEVICE)\n\n    t_h, E_h = _tokens_and_embs(pred)\n    if E_h is None:\n        return None\n    w_h = _idf_like_weights(t_h)\n\n    return _emd_cosine_score(E_r, E_h, w_r, w_h)\n\n# ---- sanity check ----\nprint(\"MoverScore sanity:\",\n      compute_moverscore(\"A fast brown fox leaps over a lazy dog.\",\n                         \"The quick brown fox jumps over the lazy dog.\"))\n\n# ===============================\n# Other metrics: ROUGE, BLEU/SacreBLEU, METEOR\n# ===============================\nrouge = load(\"rouge\")\n\n# BLEU / SacreBLEU\ntry:\n    sacrebleu = load(\"sacrebleu\")\n    use_sacre_bleu = True\nexcept Exception:\n    sacrebleu = None\n    use_sacre_bleu = False\n    bleu = load(\"bleu\")\n\ndef compute_bleu(pred, ref):\n    if use_sacre_bleu:\n        return float(sacrebleu.compute(predictions=[pred], references=[[ref]]).get(\"score\", 0.0))\n    else:\n        return float(bleu.compute(predictions=[pred], references=[ref]).get(\"bleu\", 0.0))\n\n# METEOR\nmeteor = load(\"meteor\")\n\n# ===============================\n# Evaluate all model outputs in `summaries`\n# ===============================\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrecords = []\n\nprint(\"-\" * 100)\nfor name, pred in summaries.items():\n    # ROUGE\n    r = rouge.compute(predictions=[pred], references=[reference], use_stemmer=True)\n    row = {rn: r[rn] for rn in rouge_names}\n\n    # BLEU\n    row[\"bleu\"] = compute_bleu(pred, reference)\n\n    # METEOR\n    row[\"meteor\"] = meteor.compute(predictions=[pred], references=[reference])[\"meteor\"]\n\n    # MoverScore (robust)\n    row[\"moverscore\"] = compute_moverscore(pred, reference)\n\n    records.append(row)\n\n    print(f\"=== {name.upper()} ===\")\n    print(\"ROUGE-1:\", row[\"rouge1\"], \"| ROUGE-2:\", row[\"rouge2\"],\n          \"| ROUGE-L:\", row[\"rougeL\"], \"| ROUGE-Lsum:\", row[\"rougeLsum\"])\n    print(\"BLEU:\", row[\"bleu\"], \"| METEOR:\", row[\"meteor\"],\n          \"| MoverScore:\", row[\"moverscore\"])\n    print(\"-\" * 100)\n\ndf = pd.DataFrame.from_records(records, index=list(summaries.keys()))\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:01:28.145649Z","iopub.execute_input":"2025-09-15T18:01:28.146060Z","iopub.status.idle":"2025-09-15T18:02:18.821574Z","shell.execute_reply.started":"2025-09-15T18:01:28.146026Z","shell.execute_reply":"2025-09-15T18:02:18.820453Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.2/562.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c522a30f0bad4eb0bd3bc3d2a820bb53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85d25f241e4b40af95f715ebfe828be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"173c67caf7064202978a8e0f11d546e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52fc4d64601a4d859b23d96323d10cee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6407c88eeb496bafc24e28510d8236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec918f2fb1684428b1a564747bfebadb"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[MoverScore encoder] roberta-large on cpu\nMoverScore sanity: 0.9903908003092716\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\n=== BASELINE ===\nROUGE-1: 0.5783132530120482 | ROUGE-2: 0.5344129554655871 | ROUGE-L: 0.30522088353413657 | ROUGE-Lsum: 0.30522088353413657\nBLEU: 0.34394239843199864 | METEOR: 0.6595838747418761 | MoverScore: 0.9628136720491607\n----------------------------------------------------------------------------------------------------\n=== XLNET ===\nROUGE-1: 0.30607966457023067 | ROUGE-2: 0.27789473684210525 | ROUGE-L: 0.15932914046121593 | ROUGE-Lsum: 0.15932914046121593\nBLEU: 0.14338090441907075 | METEOR: 0.46819060365053206 | MoverScore: 0.953999970589575\n----------------------------------------------------------------------------------------------------\n=== GPT2 ===\nROUGE-1: 0.25806451612903225 | ROUGE-2: 0.04918032786885246 | ROUGE-L: 0.1935483870967742 | ROUGE-Lsum: 0.16129032258064516\nBLEU: 0.0 | METEOR: 0.0998463901689708 | MoverScore: 0.9350118399628018\n----------------------------------------------------------------------------------------------------\n=== T5 ===\nROUGE-1: 0.48854961832061067 | ROUGE-2: 0.3875968992248062 | ROUGE-L: 0.366412213740458 | ROUGE-Lsum: 0.366412213740458\nBLEU: 0.26553784290661075 | METEOR: 0.3883823032759203 | MoverScore: 0.9632847376495307\n----------------------------------------------------------------------------------------------------\n=== BIGBIRD_PEGASUS ===\nROUGE-1: 0.12295081967213115 | ROUGE-2: 0.04132231404958677 | ROUGE-L: 0.09836065573770493 | ROUGE-Lsum: 0.10655737704918034\nBLEU: 0.0 | METEOR: 0.13986089791953935 | MoverScore: 0.9379449819600686\n----------------------------------------------------------------------------------------------------\n=== BART ===\nROUGE-1: 0.48739495798319327 | ROUGE-2: 0.3931623931623931 | ROUGE-L: 0.38655462184873945 | ROUGE-Lsum: 0.38655462184873945\nBLEU: 0.2378359977805848 | METEOR: 0.33466652056395646 | MoverScore: 0.9629484817894548\n----------------------------------------------------------------------------------------------------\n=== DISTILBART ===\nROUGE-1: 0.5037037037037037 | ROUGE-2: 0.37593984962406013 | ROUGE-L: 0.35555555555555557 | ROUGE-Lsum: 0.35555555555555557\nBLEU: 0.24975628790024082 | METEOR: 0.39489565136868804 | MoverScore: 0.9651903507694878\n----------------------------------------------------------------------------------------------------\n=== PEGASUS ===\nROUGE-1: 0.4201680672268907 | ROUGE-2: 0.3247863247863248 | ROUGE-L: 0.3361344537815126 | ROUGE-Lsum: 0.3361344537815126\nBLEU: 0.16434332711829372 | METEOR: 0.2789181907951228 | MoverScore: 0.96422206410471\n----------------------------------------------------------------------------------------------------\n=== LED ===\nROUGE-1: 0.576 | ROUGE-2: 0.5241935483870968 | ROUGE-L: 0.304 | ROUGE-Lsum: 0.304\nBLEU: 0.33542009143623164 | METEOR: 0.6587305967667897 | MoverScore: 0.963083346807766\n----------------------------------------------------------------------------------------------------\n=== PROPHETNET ===\nROUGE-1: 0.13824884792626727 | ROUGE-2: 0.037209302325581395 | ROUGE-L: 0.08294930875576037 | ROUGE-Lsum: 0.08294930875576037\nBLEU: 0.0 | METEOR: 0.11838061465721042 | MoverScore: 0.9435488455264777\n----------------------------------------------------------------------------------------------------\n                   rouge1    rouge2    rougeL  rougeLsum      bleu    meteor  \\\nbaseline         0.578313  0.534413  0.305221   0.305221  0.343942  0.659584   \nxlnet            0.306080  0.277895  0.159329   0.159329  0.143381  0.468191   \ngpt2             0.258065  0.049180  0.193548   0.161290  0.000000  0.099846   \nt5               0.488550  0.387597  0.366412   0.366412  0.265538  0.388382   \nbigbird_pegasus  0.122951  0.041322  0.098361   0.106557  0.000000  0.139861   \nbart             0.487395  0.393162  0.386555   0.386555  0.237836  0.334667   \ndistilbart       0.503704  0.375940  0.355556   0.355556  0.249756  0.394896   \npegasus          0.420168  0.324786  0.336134   0.336134  0.164343  0.278918   \nled              0.576000  0.524194  0.304000   0.304000  0.335420  0.658731   \nprophetnet       0.138249  0.037209  0.082949   0.082949  0.000000  0.118381   \n\n                 moverscore  \nbaseline           0.962814  \nxlnet              0.954000  \ngpt2               0.935012  \nt5                 0.963285  \nbigbird_pegasus    0.937945  \nbart               0.962948  \ndistilbart         0.965190  \npegasus            0.964222  \nled                0.963083  \nprophetnet         0.943549  \n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# # Combine BBC News + CNN/DailyMail to 2,200 rows (1,100 each)\n# # Robust to column-name variants.\n\n# import os\n# import pandas as pd\n\n# # ====== CONFIG ======\n# BBC_CSV_PATH = \"/kaggle/input/bbc-news-dataset/bbc_news_summary.csv\"\n# CNN_CSV_PATH = \"/kaggle/input/cnn-dataser/cnn_dailymail_summary.csv\"  # set to your file, or None to use HF\n# SAMPLES_PER_DATASET = 1100\n# OUTPUT_CSV = \"/kaggle/working/bbc_cnn_2200.csv\"\n# RANDOM_STATE = 42\n\n# def _clean_str(s):\n#     return \" \".join(str(s).strip().split())\n\n# def _standardize_cols(df, article_candidates, summary_candidates):\n#     # find actual column names case-insensitively\n#     lower_map = {c.lower(): c for c in df.columns}\n#     art_name = next((lower_map[c] for c in article_candidates if c in lower_map), None)\n#     sum_name = next((lower_map[c] for c in summary_candidates if c in lower_map), None)\n#     if not art_name or not sum_name:\n#         raise ValueError(\n#             f\"Could not find article/summary columns.\\n\"\n#             f\"Looked for article in {article_candidates} and summary in {summary_candidates}.\\n\"\n#             f\"Found columns: {list(df.columns)}\"\n#         )\n#     df = df[[art_name, sum_name]].rename(columns={art_name: \"article\", sum_name: \"highlights\"})\n#     df.dropna(subset=[\"article\", \"highlights\"], inplace=True)\n#     df[\"article\"] = df[\"article\"].map(_clean_str)\n#     df[\"highlights\"] = df[\"highlights\"].map(_clean_str)\n#     # remove very short items\n#     df = df[(df[\"article\"].str.len() > 30) & (df[\"highlights\"].str.len() > 10)]\n#     return df\n\n# # --------- Load BBC (handles both {Articles,Summaries} and {Text,Summary}) ----------\n# def load_bbc(path: str) -> pd.DataFrame:\n#     if not os.path.exists(path):\n#         raise FileNotFoundError(f\"BBC CSV not found at: {path}\")\n#     df = pd.read_csv(path)\n#     df = _standardize_cols(\n#         df,\n#         article_candidates=(\"articles\", \"text\"),      # accept either\n#         summary_candidates=(\"summaries\", \"summary\"),\n#     )\n#     df[\"source\"] = \"bbc\"\n#     return df\n\n# # --------- Load CNN/DailyMail (handles 'article' or 'articles') ----------\n# def load_cnn_dm(csv_path: str | None) -> pd.DataFrame:\n#     if csv_path and os.path.exists(csv_path):\n#         df = pd.read_csv(csv_path)\n#         df = _standardize_cols(\n#             df,\n#             article_candidates=(\"article\", \"articles\"),\n#             summary_candidates=(\"highlights\", \"summary\", \"summaries\"),\n#         )\n#         df[\"source\"] = \"cnn_dailymail\"\n#         return df\n#     else:\n#         # Fallback to Hugging Face if no CSV provided\n#         from datasets import load_dataset\n#         ds = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n#         pdf = ds.to_pandas()\n#         pdf = _standardize_cols(\n#             pdf,\n#             article_candidates=(\"article\", \"articles\"),\n#             summary_candidates=(\"highlights\", \"summary\", \"summaries\"),\n#         )\n#         pdf[\"source\"] = \"cnn_dailymail\"\n#         return pdf\n\n# # --------- Main combine logic ----------\n# bbc = load_bbc(BBC_CSV_PATH)\n# cnn = load_cnn_dm(CNN_CSV_PATH)\n\n# # Ensure enough rows exist\n# if len(bbc) < SAMPLES_PER_DATASET:\n#     raise ValueError(f\"BBC has only {len(bbc)} valid rows after cleaning; need {SAMPLES_PER_DATASET}.\")\n# if len(cnn) < SAMPLES_PER_DATASET:\n#     raise ValueError(f\"CNN/DM has only {len(cnn)} valid rows after cleaning; need {SAMPLES_PER_DATASET}.\")\n\n# # Sample 1,100 from each\n# bbc_sample = bbc.sample(n=SAMPLES_PER_DATASET, random_state=RANDOM_STATE)\n# cnn_sample = cnn.sample(n=SAMPLES_PER_DATASET, random_state=RANDOM_STATE)\n\n# # Concatenate, shuffle, save\n# combined = pd.concat([bbc_sample, cnn_sample], ignore_index=True)\n# combined = combined.sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n\n# os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n# combined.to_csv(OUTPUT_CSV, index=False)\n\n# print(\"Done!\")\n# print(f\"BBC rows used: {len(bbc_sample)} | CNN/DM rows used: {len(cnn_sample)}\")\n# print(f\"Combined shape: {combined.shape}\")\n# print(f\"Saved to: {OUTPUT_CSV}\")\n# print(\"\\nPreview:\")\n# print(combined.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:02:18.823940Z","iopub.execute_input":"2025-09-15T18:02:18.824271Z","iopub.status.idle":"2025-09-15T18:02:18.832585Z","shell.execute_reply.started":"2025-09-15T18:02:18.824239Z","shell.execute_reply":"2025-09-15T18:02:18.831299Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# import os, re, time, warnings\n# from typing import Dict, List, Tuple, Optional\n# import torch\n# from transformers import logging\n# logging.set_verbosity_error()\n# warnings.filterwarnings(\"ignore\")\n# import pandas as pd\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# # --- Use your ~100-word article here ---\n# ARTICLE_TEXT = \"\"\"\n# Peshawar's Institute of Management Sciences (IMSciences) has been active with student-focused initiatives. \n# The university extended its application deadline for all programs, giving applicants more time to apply. \n# It also hosted a business fest where students presented innovative projects linked to the Sustainable Development Goals (SDGs). \n# These efforts reflect IMSciences' focus on academic excellence, entrepreneurship, and community engagement. \n# The university continues to improve its academic programs and campus facilities to provide a supportive learning environment, \n# while a video from earlier years highlights the vibrant campus and student life at IMSciences.\n# \"\"\".strip()\n\n# CSV_PATH = \"imsciences_qualitative_results_short.csv\"\n\n# MODEL_IDS: Dict[str, str] = {\n#     \"BART\": \"facebook/bart-large-cnn\",\n#     \"DistilBART\": \"sshleifer/distilbart-cnn-12-6\",\n#     \"T5\": \"t5-base\",\n#     \"PEGASUS\": \"google/pegasus-cnn_dailymail\",\n#     \"LED\": \"allenai/led-base-16384\",\n#     \"BigBird-Pegasus\": \"google/bigbird-pegasus-large-pubmed\",  # still off-domain but ok to include\n#     \"GPT-2\": \"gpt2\",                      # baseline (not a summarizer)\n#     \"XLNet\": \"xlnet-base-cased\",          # baseline (not a summarizer)\n# }\n\n# # ---- STRICT LENGTH TARGETS (tweak here) ----\n# MAX_SENTENCES = 4          # hard cap on sentences\n# MAX_WORDS = 80             # hard cap on words (after sentence cap)\n# MIN_NEW_TOKENS = 35        # model-level min generation\n# MAX_NEW_TOKENS = 90        # model-level max generation (tight!)\n# LENGTH_PENALTY = 2.0       # >1 penalizes long outputs\n# NO_REPEAT_NGRAM = 3\n\n# # ---------- helpers ----------\n# def _clean_summary(s: str) -> str:\n#     s = s.replace(\"<n>\", \" \")\n#     s = re.sub(r\"\\s+\", \" \", s).strip()\n#     s = re.sub(r\"(\\b\\w+\\b)(?:\\s+\\1\\b){3,}\", r\"\\1\", s, flags=re.I)  # collapse w w w w\n#     return s\n\n# def _limit_length(text: str, max_sentences=MAX_SENTENCES, max_words=MAX_WORDS) -> str:\n#     # keep only first N sentences\n#     sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n#     sents = [s for s in sents if s]\n#     sents = sents[:max_sentences]\n#     clipped = \" \".join(sents)\n#     # enforce word cap\n#     words = clipped.split()\n#     if len(words) > max_words:\n#         clipped = \" \".join(words[:max_words]).rstrip(\",;:\") + \"...\"\n#     return clipped\n\n# def _flags(text: str) -> dict:\n#     rep_triplets = bool(re.search(r\"(\\b\\w+\\b)(?:\\s+\\1\\b){2,}\", text, flags=re.I))\n#     long_run = max([len(run) for run in re.split(r\"[.!?]\", text)]) > 300 if text else False\n#     has_code = (\"@\" in text) or (\"<\" in text and \">\" in text)\n#     return {\"flag_repetition\": int(rep_triplets), \"flag_long_run\": int(long_run), \"flag_code_tokens\": int(has_code)}\n\n# def _chunk(tok, text: str, max_input_tokens: Optional[int] = None, margin: int = 64):\n#     if max_input_tokens is None:\n#         raw = getattr(tok, \"model_max_length\", 1024)\n#         max_input_tokens = min(int(raw), 16384)\n#     enc = tok(text, return_tensors=\"pt\", truncation=False, add_special_tokens=True)\n#     ids = enc[\"input_ids\"][0]; attn = enc[\"attention_mask\"][0]\n#     limit = max(128, max_input_tokens - margin)\n#     batches = []\n#     for st in range(0, ids.shape[0], limit):\n#         ed = min(st + limit, ids.shape[0])\n#         batches.append({\"input_ids\": ids[st:ed].unsqueeze(0).to(DEVICE),\n#                         \"attention_mask\": attn[st:ed].unsqueeze(0).to(DEVICE)})\n#     return batches\n\n# @torch.inference_mode()\n# def _summarize_seq2seq(model, tok, article: str, max_in: int,\n#                        min_new=MIN_NEW_TOKENS, max_new=MAX_NEW_TOKENS,\n#                        beams=4, led_global=False, repetition_penalty=1.15):\n#     chunks = _chunk(tok, article, max_input_tokens=max_in)\n#     total_in = total_out = 0\n#     parts = []\n#     for b in chunks:\n#         total_in += int(b[\"input_ids\"].shape[-1])\n#         kwargs = dict(\n#             min_new_tokens=min_new, max_new_tokens=max_new,\n#             num_beams=beams, length_penalty=LENGTH_PENALTY,\n#             no_repeat_ngram_size=NO_REPEAT_NGRAM,\n#             encoder_no_repeat_ngram_size=NO_REPEAT_NGRAM,\n#             early_stopping=True, repetition_penalty=repetition_penalty\n#         )\n#         if led_global:\n#             g = torch.zeros_like(b[\"attention_mask\"]); g[:, 0] = 1\n#             ids = model.generate(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"],\n#                                  global_attention_mask=g, **kwargs)\n#         else:\n#             ids = model.generate(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"], **kwargs)\n#         total_out += int(ids.shape[-1])\n#         parts.append(_clean_summary(tok.decode(ids[0], skip_special_tokens=True)))\n\n#     out = \" \".join(parts).strip()\n\n#     # If multiple chunks → compress again, but with even stricter cap\n#     if len(chunks) > 1:\n#         kwargs2 = dict(\n#             min_new_tokens=max(10, min_new // 2),\n#             max_new_tokens=max(40, max_new // 2),\n#             num_beams=beams, length_penalty=LENGTH_PENALTY,\n#             no_repeat_ngram_size=NO_REPEAT_NGRAM,\n#             encoder_no_repeat_ngram_size=NO_REPEAT_NGRAM,\n#             early_stopping=True, repetition_penalty=repetition_penalty\n#         )\n#         inp = tok(out, return_tensors=\"pt\", truncation=True, max_length=max_in).to(DEVICE)\n#         ids = model.generate(**inp, **kwargs2)\n#         total_out += int(ids.shape[-1])\n#         out = _clean_summary(tok.decode(ids[0], skip_special_tokens=True))\n\n#     # Hard final clip\n#     out = _limit_length(out)\n#     return out, total_in, total_out\n\n# @torch.inference_mode()\n# def _summarize_causal(model, tok, article: str, max_input=1024,\n#                       min_new=MIN_NEW_TOKENS, max_new=MAX_NEW_TOKENS, repetition_penalty=1.2):\n#     prompt = (\n#         \"Summarize the news article in 3–4 concise sentences. Be factual and avoid repetition.\\n\\n\"\n#         f\"Article:\\n{article}\\n\\nSummary:\"\n#     )\n#     inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input).to(DEVICE)\n#     eos = tok.eos_token_id if tok.eos_token_id is not None else 50256\n#     ids = model.generate(\n#         **inputs, do_sample=False,\n#         min_new_tokens=min_new, max_new_tokens=max_new,\n#         length_penalty=LENGTH_PENALTY, no_repeat_ngram_size=NO_REPEAT_NGRAM,\n#         repetition_penalty=repetition_penalty, pad_token_id=eos, eos_token_id=eos\n#     )\n#     text = tok.decode(ids[0], skip_special_tokens=True)\n#     out = text.split(\"Summary:\", 1)[-1].strip()\n#     if \".\" in out: out = out.rsplit(\".\", 1)[0] + \".\"\n#     out = _clean_summary(out)\n#     out = _limit_length(out)\n#     return out, int(inputs[\"input_ids\"].shape[-1]), int(ids.shape[-1])\n\n# def run_panel(article: str) -> pd.DataFrame:\n#     rows = []\n#     for name, model_id in MODEL_IDS.items():\n#         print(f\"\\n=== {name} ({model_id}) ===\")\n#         t0 = time.time()\n#         tok = AutoTokenizer.from_pretrained(model_id)\n#         max_in = getattr(tok, \"model_max_length\", 1024)\n#         if max_in > 2_000_000_000: max_in = 4096 if \"led\" in model_id.lower() else 1024\n\n#         # try seq2seq first; fall back to causal\n#         is_seq2seq = True\n#         try:\n#             mdl = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(DEVICE)\n#         except Exception:\n#             is_seq2seq = False\n#             mdl = AutoModelForCausalLM.from_pretrained(model_id).to(DEVICE)\n\n#         if is_seq2seq:\n#             text = f\"summarize: {article}\" if \"t5\" in model_id.lower() else article\n#             led_mode = (\"led\" in model_id.lower())\n#             summary, tin, tout = _summarize_seq2seq(mdl, tok, text, max_in)\n#         else:\n#             summary, tin, tout = _summarize_causal(mdl, tok, article, max_input=min(max_in, 1024))\n\n#         dt = round(time.time() - t0, 2)\n#         flags = _flags(summary)\n#         rows.append({\n#             \"model\": name, \"model_id\": model_id, \"time_sec\": dt,\n#             \"tokens_in\": tin, \"tokens_out\": tout, **flags, \"summary\": summary\n#         })\n#     cols = [\"model\",\"model_id\",\"time_sec\",\"tokens_in\",\"tokens_out\",\n#             \"flag_repetition\",\"flag_long_run\",\"flag_code_tokens\",\"summary\"]\n#     return pd.DataFrame(rows, columns=cols)\n\n# df_short = run_panel(ARTICLE_TEXT)\n# df_short.to_csv(CSV_PATH, index=False, encoding=\"utf-8\")\n# print(f\"\\n[OK] CSV saved → {os.path.abspath(CSV_PATH)}\")\n# df_short\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:02:18.834046Z","iopub.execute_input":"2025-09-15T18:02:18.834419Z","iopub.status.idle":"2025-09-15T18:02:18.863553Z","shell.execute_reply.started":"2025-09-15T18:02:18.834383Z","shell.execute_reply":"2025-09-15T18:02:18.862430Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# ===============================\n# Install dependencies (Kaggle safe)\n# ===============================\n!pip install --quiet nltk rouge-score bert-score\n\n# ===============================\n# Imports\n# ===============================\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import single_meteor_score\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nfrom nltk.tokenize import word_tokenize\n\n# Download necessary NLTK data\nnltk.download('punkt', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\n\n# ===============================\n# Load CSV safely with fallback encoding\n# ===============================\nINPUT_CSV = \"/kaggle/input/statistical/statistical.csv\"   # <-- change path\ntry:\n    df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\nexcept UnicodeDecodeError:\n    try:\n        df = pd.read_csv(INPUT_CSV, encoding=\"latin1\")\n    except:\n        df = pd.read_csv(INPUT_CSV, encoding=\"ISO-8859-1\")\n\nprint(f\"[INFO] Loaded dataset: {INPUT_CSV}  shape={df.shape}\")\nprint(f\"[INFO] Columns: {list(df.columns)}\")\n\n# ===============================\n# Utility Functions\n# ===============================\n\n# ROUGE per-document (returns dicts)\ndef rouge_per_doc(preds, refs):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = []\n    for p, r in zip(preds, refs):\n        res = scorer.score(r, p)\n        scores.append({\n            \"rouge1\": res['rouge1'].fmeasure,\n            \"rouge2\": res['rouge2'].fmeasure,\n            \"rougeL\": res['rougeL'].fmeasure\n        })\n    return scores\n\n# BLEU per-document\ndef bleu_per_doc(preds, refs):\n    smooth = SmoothingFunction().method1\n    return [sentence_bleu([r.split()], p.split(), smoothing_function=smooth) for p, r in zip(preds, refs)]\n\n# METEOR per-document (tokenized fix)\ndef meteor_per_doc(preds, refs):\n    scores = []\n    for p, r in zip(preds, refs):\n        r_tok, p_tok = word_tokenize(r), word_tokenize(p)\n        scores.append(single_meteor_score(r_tok, p_tok))\n    return scores\n\n# BERTScore per-document\ndef bert_f1_per_doc(preds, refs, lang=\"en\"):\n    P, R, F1 = bert_score(preds, refs, lang=lang, verbose=False)\n    return F1.tolist()\n\n# ===============================\n# Compute Scores\n# ===============================\nmodels = [\"LED\", \"DistilBART\", \"BART\", \"T5\"]   # pick your models from CSV\nrecords = []\n\nfor _, row in df.iterrows():\n    ref = str(row[\"reference\"])  # column for gold summary\n    for m in models:\n        hyp = str(row[m])\n        records.append({\"doc_id\": row.get(\"doc_id\", _), \"model\": m, \"reference\": ref, \"prediction\": hyp})\n\ndf_long = pd.DataFrame(records)\n\n# Compute metrics grouped by doc/model\nresults = []\nfor model in models:\n    subset = df_long[df_long[\"model\"] == model]\n    preds, refs = subset[\"prediction\"].tolist(), subset[\"reference\"].tolist()\n\n    # Compute all metrics\n    rouge_scores = rouge_per_doc(preds, refs)\n    bleu_scores = bleu_per_doc(preds, refs)\n    meteor_scores = meteor_per_doc(preds, refs)\n    bert_scores = bert_f1_per_doc(preds, refs)\n\n    # Build DataFrame\n    part = pd.DataFrame(rouge_scores)\n    part[\"BLEU\"] = bleu_scores\n    part[\"METEOR\"] = meteor_scores\n    part[\"BERTScore\"] = bert_scores\n    part[\"model\"] = model\n    results.append(part)\n\ndf_metrics = pd.concat(results, ignore_index=True)\n\n# ===============================\n# Save + Preview\n# ===============================\ndf_metrics.to_csv(\"per_document_metrics.csv\", index=False)\nprint(\"[INFO] Saved per-document metrics → per_document_metrics.csv\")\ndf_metrics.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:37:43.685796Z","iopub.execute_input":"2025-09-15T18:37:43.686241Z","iopub.status.idle":"2025-09-15T18:38:18.274384Z","shell.execute_reply.started":"2025-09-15T18:37:43.686210Z","shell.execute_reply":"2025-09-15T18:38:18.273162Z"}},"outputs":[{"name":"stdout","text":"[INFO] Loaded dataset: /kaggle/input/statistical/statistical.csv  shape=(48, 6)\n[INFO] Columns: ['doc_id', 'reference', 'LED', 'DistilBART', 'BART', 'T5']\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[INFO] Saved per-document metrics → per_document_metrics.csv\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"     rouge1    rouge2    rougeL      BLEU    METEOR  BERTScore model\n0  0.923077  0.545455  0.769231  0.194331  0.851201   0.906519   LED\n1  0.461538  0.181818  0.307692  0.034557  0.399525   0.950372   LED\n2  0.363636  0.000000  0.363636  0.043989  0.370370   0.882310   LED\n3  0.461538  0.181818  0.461538  0.080876  0.399525   0.912072   LED\n4  0.500000  0.333333  0.500000  0.058335  0.440613   0.906364   LED","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>BLEU</th>\n      <th>METEOR</th>\n      <th>BERTScore</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.923077</td>\n      <td>0.545455</td>\n      <td>0.769231</td>\n      <td>0.194331</td>\n      <td>0.851201</td>\n      <td>0.906519</td>\n      <td>LED</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.461538</td>\n      <td>0.181818</td>\n      <td>0.307692</td>\n      <td>0.034557</td>\n      <td>0.399525</td>\n      <td>0.950372</td>\n      <td>LED</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.363636</td>\n      <td>0.000000</td>\n      <td>0.363636</td>\n      <td>0.043989</td>\n      <td>0.370370</td>\n      <td>0.882310</td>\n      <td>LED</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.461538</td>\n      <td>0.181818</td>\n      <td>0.461538</td>\n      <td>0.080876</td>\n      <td>0.399525</td>\n      <td>0.912072</td>\n      <td>LED</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.500000</td>\n      <td>0.058335</td>\n      <td>0.440613</td>\n      <td>0.906364</td>\n      <td>LED</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# =========================================================\n# Significance testing + summary from per-document metrics\n# Works with columns: ['rouge1','rouge2','rougeL','BLEU','METEOR','BERTScore','model']\n# and one row per (doc, model). If you have a 'doc_id' column, it's used automatically.\n# =========================================================\nimport os, numpy as np, pandas as pd\nfrom itertools import combinations\nfrom scipy.stats import ttest_rel, wilcoxon\n\n# ---- Load metrics table ----\nif 'df_metrics' in globals():\n    dfm = df_metrics.copy()\nelse:\n    # Try the file saved by your previous step\n    path = 'per_document_metrics.csv'\n    if not os.path.exists(path):\n        raise FileNotFoundError(\"Could not find per_document_metrics.csv and df_metrics is not defined.\")\n    dfm = pd.read_csv(path)\n\n# Normalize column names (just in case)\ndfm.columns = [c.strip() for c in dfm.columns]\n# Try to guess doc_id if missing\nif 'doc_id' not in dfm.columns:\n    dfm['doc_id'] = dfm.groupby('model').cumcount() + 1\n\n# Metrics present (harmonize names)\nname_map = {\n    'rouge1':'ROUGE1','rouge2':'ROUGE2','rougeL':'ROUGEL',\n    'BLEU':'BLEU','METEOR':'METEOR','BERTScore':'BERTScoreF1',\n    'ROUGE-1':'ROUGE1','ROUGE-2':'ROUGE2','ROUGE-L':'ROUGEL'\n}\ndfm = dfm.rename(columns={k:v for k,v in name_map.items() if k in dfm.columns})\nMETRICS = [m for m in ['ROUGE1','ROUGE2','ROUGEL','BLEU','METEOR','BERTScoreF1'] if m in dfm.columns]\n\nMODELS = sorted(dfm['model'].unique().tolist())\nprint(f\"[INFO] Models: {MODELS}\")\nprint(f\"[INFO] Metrics: {METRICS}\")\nn_docs = dfm['doc_id'].nunique()\nprint(f\"[INFO] Number of documents: {n_docs}\")\n\n# ---- Helper: paired Cohen's d ----\ndef cohens_d_paired(a, b):\n    # a,b are paired arrays\n    diff = a - b\n    sd = diff.std(ddof=1)\n    if sd == 0:\n        return 0.0\n    return diff.mean() / sd\n\n# ---- Pairwise tests for all model pairs and metrics ----\nrows = []\nfor metric in METRICS:\n    for m1, m2 in combinations(MODELS, 2):\n        a = (dfm[dfm['model']==m1]\n             .sort_values('doc_id')[metric].astype(float).to_numpy())\n        b = (dfm[dfm['model']==m2]\n             .sort_values('doc_id')[metric].astype(float).to_numpy())\n        # Align lengths if anything went off (shouldn't, but safe):\n        n = min(len(a), len(b))\n        a, b = a[:n], b[:n]\n        # T-test\n        t_stat, p_t = ttest_rel(a, b)\n        # Wilcoxon (may fail if all diffs are zero)\n        try:\n            w_stat, p_w = wilcoxon(a, b, zero_method='wilcox')\n        except ValueError:\n            w_stat, p_w = np.nan, np.nan\n        d = cohens_d_paired(a, b)\n        rows.append({\n            'metric': metric, 'model_A': m1, 'model_B': m2,\n            'n_docs': n, 'mean_A': float(a.mean()), 'mean_B': float(b.mean()),\n            'cohens_d(A-B)': float(d),\n            't_stat': float(t_stat), 'p_t': float(p_t),\n            'w_stat': (np.nan if isinstance(w_stat,float) and np.isnan(w_stat) else float(w_stat)),\n            'p_w': (np.nan if isinstance(p_w,float) and np.isnan(p_w) else float(p_w)),\n        })\nsig_df = pd.DataFrame(rows)\n\n# ---- Multiple-comparison correction (Holm) per metric ----\ndef holm_bonferroni(pvals):\n    # returns adjusted p-values in original order\n    m = len(pvals)\n    order = np.argsort(pvals)\n    adj = np.empty(m)\n    prev = 0.0\n    for i, idx in enumerate(order):\n        rank = m - i\n        adj_val = pvals[idx] * rank\n        adj_val = max(adj_val, prev)  # ensure monotonicity\n        adj[idx] = min(adj_val, 1.0)\n        prev = adj[idx]\n    return adj\n\nsig_df['pmin'] = np.nanmin(sig_df[['p_t','p_w']].values, axis=1)\nadj_rows = []\nfor metric, g in sig_df.groupby('metric'):\n    adj = holm_bonferroni(g['pmin'].values)\n    tmp = g.copy()\n    tmp['pmin_holm'] = adj\n    adj_rows.append(tmp)\nsig_df = pd.concat(adj_rows, ignore_index=True)\n\n# Significance flags\nsig_df['sig_t(p<0.05)'] = sig_df['p_t'] < 0.05\nsig_df['sig_w(p<0.05)'] = sig_df['p_w'] < 0.05\nsig_df['sig_holm(p<0.05)'] = sig_df['pmin_holm'] < 0.05\n\nsig_df = sig_df.sort_values(['metric','pmin_holm']).reset_index(drop=True)\nsig_df.to_csv('significance_results.csv', index=False)\nprint(\"[OK] Saved -> significance_results.csv\")\ndisplay(sig_df.head(12))\n\n# ---- Summary table: mean ± sd per model/metric ----\nsumm_rows = []\nfor metric in METRICS:\n    for m in MODELS:\n        vals = (dfm[dfm['model']==m]\n                .sort_values('doc_id')[metric].astype(float).to_numpy())\n        summ_rows.append({\n            'metric': metric,\n            'model': m,\n            'mean': float(vals.mean()),\n            'std': float(vals.std(ddof=1)),\n            'median': float(np.median(vals))\n        })\nsummary_df = pd.DataFrame(summ_rows).sort_values(['metric','mean'], ascending=[True, False])\nsummary_df.to_csv('model_metric_summary.csv', index=False)\nprint(\"[OK] Saved -> model_metric_summary.csv\")\ndisplay(summary_df.head(12))\n\n# ---- Quick, human-readable recap by metric: who is best and who it beats after Holm ----\nprint(\"\\n=== Compact recap (Holm-corrected, using min p of t/Wilcoxon) ===\")\nfor metric in METRICS:\n    best = summary_df[summary_df['metric']==metric].iloc[0]\n    best_model = best['model']\n    best_mean = best['mean']\n    print(f\"\\n[{metric}] best mean = {best_mean:.3f} → {best_model}\")\n    g = sig_df[(sig_df['metric']==metric)]\n    wins = []\n    for _, r in g.iterrows():\n        # If pair includes best, check if best is A or B and whether adj p < .05\n        if best_model in (r['model_A'], r['model_B']) and r['sig_holm(p<0.05)']:\n            other = r['model_B'] if r['model_A']==best_model else r['model_A']\n            # Determine direction (is best actually higher on average?)\n            if best_model == r['model_A'] and r['mean_A'] >= r['mean_B']:\n                wins.append(other)\n            elif best_model == r['model_B'] and r['mean_B'] >= r['mean_A']:\n                wins.append(other)\n    if wins:\n        print(f\"  Significant vs: {', '.join(sorted(set(wins)))} (Holm p<.05)\")\n    else:\n        print(\"  No significant wins after Holm correction.\")\n\n# ---- Ready-to-paste one-liners (uncorrected and corrected) ----\nprint(\"\\n=== Suggested sentences ===\")\nfor _, r in sig_df.iterrows():\n    direction = \"better\" if r['mean_A'] > r['mean_B'] else \"worse\"\n    print(f\"- {r['model_A']} vs {r['model_B']} on {r['metric']}: \"\n          f\"{direction}; paired t p={r['p_t']:.4f}, Wilcoxon p={r['p_w']:.4f}, \"\n          f\"Holm-adjusted p={r['pmin_holm']:.4f}, d={r['cohens_d(A-B)']:.2f} (n={int(r['n_docs'])}).\")\n\n# ---- (Optional) LaTeX table of p-values vs a chosen 'best' model ----\nBEST_MODEL = summary_df.groupby('metric').apply(lambda g: g.iloc[0]['model']).to_dict()\n# If you prefer to force one best model, set e.g.: BEST_MODEL = {m: 'LED' for m in METRICS}\n\nlatex_blocks = []\nfor metric in METRICS:\n    chosen = BEST_MODEL[metric] if isinstance(BEST_MODEL, dict) else BEST_MODEL\n    block = sig_df[(sig_df['metric']==metric) & ((sig_df['model_A']==chosen)|(sig_df['model_B']==chosen))].copy()\n    block['vs'] = np.where(block['model_A']==chosen, block['model_B'], block['model_A'])\n    block = block[['vs','mean_A','mean_B','p_t','p_w','pmin_holm','cohens_d(A-B)']]\n    block = block.rename(columns={'p_t':'t_p','p_w':'w_p','pmin_holm':'holm_p','cohens_d(A-B)':'cohen_d'})\n    block = block.sort_values('holm_p')\n    # Build LaTeX\n    lines = [f\"\\\\begin{{table}}[ht]\\\\centering\",\n             f\"\\\\caption{{Significance vs {chosen} on {metric}}}\",\n             f\"\\\\label{{tab:sig_{metric.lower()}}}\",\n             f\"\\\\begin{{tabular}}{{lrrrrr}}\\\\toprule\",\n             f\"Model & t p & Wilcoxon p & Holm p & d (paired) \\\\\\\\ \\\\midrule\"]\n    for _, r in block.iterrows():\n        lines.append(f\"{r['vs']} & {r['t_p']:.4f} & {r['w_p']:.4f} & {r['holm_p']:.4f} & {r['cohen_d']:.2f} \\\\\\\\\")\n    lines += [\"\\\\bottomrule\",\"\\\\end{tabular}\",\"\\\\end{table}\"]\n    latex_blocks.append(\"\\n\".join(lines))\n\nprint(\"\\n=== LaTeX blocks (copy one per metric if you want) ===\")\nfor lb in latex_blocks:\n    print(lb, \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:44:45.796283Z","iopub.execute_input":"2025-09-15T18:44:45.796768Z","iopub.status.idle":"2025-09-15T18:44:46.110097Z","shell.execute_reply.started":"2025-09-15T18:44:45.796732Z","shell.execute_reply":"2025-09-15T18:44:46.109034Z"}},"outputs":[{"name":"stdout","text":"[INFO] Models: ['BART', 'DistilBART', 'LED', 'T5']\n[INFO] Metrics: ['ROUGE1', 'ROUGE2', 'ROUGEL', 'BLEU', 'METEOR', 'BERTScoreF1']\n[INFO] Number of documents: 48\n[OK] Saved -> significance_results.csv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:134: RuntimeWarning: invalid value encountered in greater\n  r_plus = np.sum((d > 0) * r, axis=-1)\n/usr/local/lib/python3.11/dist-packages/scipy/stats/_wilcoxon.py:135: RuntimeWarning: invalid value encountered in less\n  r_minus = np.sum((d < 0) * r, axis=-1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         metric     model_A     model_B  n_docs    mean_A    mean_B  \\\n0   BERTScoreF1  DistilBART         LED      48  0.890452  0.904591   \n1   BERTScoreF1         LED          T5      48  0.904591  0.891256   \n2   BERTScoreF1        BART  DistilBART      48  0.900725  0.890452   \n3   BERTScoreF1        BART          T5      48  0.900725  0.891256   \n4   BERTScoreF1        BART         LED      48  0.900725  0.904591   \n5   BERTScoreF1  DistilBART          T5      48  0.890452  0.891256   \n6          BLEU  DistilBART         LED      48  0.015414  0.036414   \n7          BLEU        BART  DistilBART      48  0.031646  0.015414   \n8          BLEU         LED          T5      48  0.036414  0.017008   \n9          BLEU        BART          T5      48  0.031646  0.017008   \n10         BLEU        BART         LED      48  0.031646  0.036414   \n11         BLEU  DistilBART          T5      48  0.015414  0.017008   \n\n    cohens_d(A-B)    t_stat       p_t  w_stat       p_w      pmin  pmin_holm  \\\n0       -0.658865 -4.564750  0.000036   215.0  0.000067  0.000036   0.000216   \n1        0.595294  4.124318  0.000150   235.0  0.000177  0.000150   0.000752   \n2        0.399520  2.767958  0.008045   297.0  0.002364  0.002364   0.009456   \n3        0.442836  3.068059  0.003570   307.0  0.003399  0.003399   0.010197   \n4       -0.173634 -1.202974  0.235011   432.0  0.111261  0.111261   0.222521   \n5       -0.036017 -0.249533  0.804036   568.0  0.842918  0.804036   0.804036   \n6       -0.436890 -3.026862  0.004002    70.0  0.007335  0.004002   0.024010   \n7        0.377064  2.612374  0.012035    80.0  0.045247  0.012035   0.060177   \n8        0.364222  2.523404  0.015063   102.0  0.021268  0.015063   0.060252   \n9        0.332211  2.301624  0.025836    96.0  0.025377  0.025377   0.076131   \n10      -0.095969 -0.664891  0.509369   147.5  0.476194  0.476194   0.952388   \n11      -0.040862 -0.283098  0.778345    91.0  0.600991  0.600991   0.952388   \n\n    sig_t(p<0.05)  sig_w(p<0.05)  sig_holm(p<0.05)  \n0            True           True              True  \n1            True           True              True  \n2            True           True              True  \n3            True           True              True  \n4           False          False             False  \n5           False          False             False  \n6            True           True              True  \n7            True           True             False  \n8            True           True             False  \n9            True           True             False  \n10          False          False             False  \n11          False          False             False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>metric</th>\n      <th>model_A</th>\n      <th>model_B</th>\n      <th>n_docs</th>\n      <th>mean_A</th>\n      <th>mean_B</th>\n      <th>cohens_d(A-B)</th>\n      <th>t_stat</th>\n      <th>p_t</th>\n      <th>w_stat</th>\n      <th>p_w</th>\n      <th>pmin</th>\n      <th>pmin_holm</th>\n      <th>sig_t(p&lt;0.05)</th>\n      <th>sig_w(p&lt;0.05)</th>\n      <th>sig_holm(p&lt;0.05)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERTScoreF1</td>\n      <td>DistilBART</td>\n      <td>LED</td>\n      <td>48</td>\n      <td>0.890452</td>\n      <td>0.904591</td>\n      <td>-0.658865</td>\n      <td>-4.564750</td>\n      <td>0.000036</td>\n      <td>215.0</td>\n      <td>0.000067</td>\n      <td>0.000036</td>\n      <td>0.000216</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BERTScoreF1</td>\n      <td>LED</td>\n      <td>T5</td>\n      <td>48</td>\n      <td>0.904591</td>\n      <td>0.891256</td>\n      <td>0.595294</td>\n      <td>4.124318</td>\n      <td>0.000150</td>\n      <td>235.0</td>\n      <td>0.000177</td>\n      <td>0.000150</td>\n      <td>0.000752</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BERTScoreF1</td>\n      <td>BART</td>\n      <td>DistilBART</td>\n      <td>48</td>\n      <td>0.900725</td>\n      <td>0.890452</td>\n      <td>0.399520</td>\n      <td>2.767958</td>\n      <td>0.008045</td>\n      <td>297.0</td>\n      <td>0.002364</td>\n      <td>0.002364</td>\n      <td>0.009456</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BERTScoreF1</td>\n      <td>BART</td>\n      <td>T5</td>\n      <td>48</td>\n      <td>0.900725</td>\n      <td>0.891256</td>\n      <td>0.442836</td>\n      <td>3.068059</td>\n      <td>0.003570</td>\n      <td>307.0</td>\n      <td>0.003399</td>\n      <td>0.003399</td>\n      <td>0.010197</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BERTScoreF1</td>\n      <td>BART</td>\n      <td>LED</td>\n      <td>48</td>\n      <td>0.900725</td>\n      <td>0.904591</td>\n      <td>-0.173634</td>\n      <td>-1.202974</td>\n      <td>0.235011</td>\n      <td>432.0</td>\n      <td>0.111261</td>\n      <td>0.111261</td>\n      <td>0.222521</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BERTScoreF1</td>\n      <td>DistilBART</td>\n      <td>T5</td>\n      <td>48</td>\n      <td>0.890452</td>\n      <td>0.891256</td>\n      <td>-0.036017</td>\n      <td>-0.249533</td>\n      <td>0.804036</td>\n      <td>568.0</td>\n      <td>0.842918</td>\n      <td>0.804036</td>\n      <td>0.804036</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>BLEU</td>\n      <td>DistilBART</td>\n      <td>LED</td>\n      <td>48</td>\n      <td>0.015414</td>\n      <td>0.036414</td>\n      <td>-0.436890</td>\n      <td>-3.026862</td>\n      <td>0.004002</td>\n      <td>70.0</td>\n      <td>0.007335</td>\n      <td>0.004002</td>\n      <td>0.024010</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BLEU</td>\n      <td>BART</td>\n      <td>DistilBART</td>\n      <td>48</td>\n      <td>0.031646</td>\n      <td>0.015414</td>\n      <td>0.377064</td>\n      <td>2.612374</td>\n      <td>0.012035</td>\n      <td>80.0</td>\n      <td>0.045247</td>\n      <td>0.012035</td>\n      <td>0.060177</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>BLEU</td>\n      <td>LED</td>\n      <td>T5</td>\n      <td>48</td>\n      <td>0.036414</td>\n      <td>0.017008</td>\n      <td>0.364222</td>\n      <td>2.523404</td>\n      <td>0.015063</td>\n      <td>102.0</td>\n      <td>0.021268</td>\n      <td>0.015063</td>\n      <td>0.060252</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>BLEU</td>\n      <td>BART</td>\n      <td>T5</td>\n      <td>48</td>\n      <td>0.031646</td>\n      <td>0.017008</td>\n      <td>0.332211</td>\n      <td>2.301624</td>\n      <td>0.025836</td>\n      <td>96.0</td>\n      <td>0.025377</td>\n      <td>0.025377</td>\n      <td>0.076131</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>BLEU</td>\n      <td>BART</td>\n      <td>LED</td>\n      <td>48</td>\n      <td>0.031646</td>\n      <td>0.036414</td>\n      <td>-0.095969</td>\n      <td>-0.664891</td>\n      <td>0.509369</td>\n      <td>147.5</td>\n      <td>0.476194</td>\n      <td>0.476194</td>\n      <td>0.952388</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>BLEU</td>\n      <td>DistilBART</td>\n      <td>T5</td>\n      <td>48</td>\n      <td>0.015414</td>\n      <td>0.017008</td>\n      <td>-0.040862</td>\n      <td>-0.283098</td>\n      <td>0.778345</td>\n      <td>91.0</td>\n      <td>0.600991</td>\n      <td>0.600991</td>\n      <td>0.952388</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"[OK] Saved -> model_metric_summary.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         metric       model      mean       std    median\n22  BERTScoreF1         LED  0.904591  0.021502  0.904531\n20  BERTScoreF1        BART  0.900725  0.027169  0.899023\n23  BERTScoreF1          T5  0.891256  0.024590  0.889325\n21  BERTScoreF1  DistilBART  0.890452  0.025618  0.886695\n14         BLEU         LED  0.036414  0.048922  0.032254\n12         BLEU        BART  0.031646  0.042345  0.000000\n15         BLEU          T5  0.017008  0.026475  0.000000\n13         BLEU  DistilBART  0.015414  0.030383  0.000000\n18       METEOR         LED  0.382654  0.155010  0.421524\n16       METEOR        BART  0.296794  0.137940  0.259180\n19       METEOR          T5  0.225901  0.109776  0.200000\n17       METEOR  DistilBART  0.191180  0.114877  0.170953","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>metric</th>\n      <th>model</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>median</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>BERTScoreF1</td>\n      <td>LED</td>\n      <td>0.904591</td>\n      <td>0.021502</td>\n      <td>0.904531</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>BERTScoreF1</td>\n      <td>BART</td>\n      <td>0.900725</td>\n      <td>0.027169</td>\n      <td>0.899023</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>BERTScoreF1</td>\n      <td>T5</td>\n      <td>0.891256</td>\n      <td>0.024590</td>\n      <td>0.889325</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>BERTScoreF1</td>\n      <td>DistilBART</td>\n      <td>0.890452</td>\n      <td>0.025618</td>\n      <td>0.886695</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>BLEU</td>\n      <td>LED</td>\n      <td>0.036414</td>\n      <td>0.048922</td>\n      <td>0.032254</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>BLEU</td>\n      <td>BART</td>\n      <td>0.031646</td>\n      <td>0.042345</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>BLEU</td>\n      <td>T5</td>\n      <td>0.017008</td>\n      <td>0.026475</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>BLEU</td>\n      <td>DistilBART</td>\n      <td>0.015414</td>\n      <td>0.030383</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>METEOR</td>\n      <td>LED</td>\n      <td>0.382654</td>\n      <td>0.155010</td>\n      <td>0.421524</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>METEOR</td>\n      <td>BART</td>\n      <td>0.296794</td>\n      <td>0.137940</td>\n      <td>0.259180</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>METEOR</td>\n      <td>T5</td>\n      <td>0.225901</td>\n      <td>0.109776</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>METEOR</td>\n      <td>DistilBART</td>\n      <td>0.191180</td>\n      <td>0.114877</td>\n      <td>0.170953</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n=== Compact recap (Holm-corrected, using min p of t/Wilcoxon) ===\n\n[ROUGE1] best mean = 0.462 → LED\n  Significant vs: DistilBART, T5 (Holm p<.05)\n\n[ROUGE2] best mean = 0.215 → LED\n  Significant vs: BART, DistilBART, T5 (Holm p<.05)\n\n[ROUGEL] best mean = 0.439 → LED\n  Significant vs: BART, DistilBART, T5 (Holm p<.05)\n\n[BLEU] best mean = 0.036 → LED\n  Significant vs: DistilBART (Holm p<.05)\n\n[METEOR] best mean = 0.383 → LED\n  Significant vs: BART, DistilBART, T5 (Holm p<.05)\n\n[BERTScoreF1] best mean = 0.905 → LED\n  Significant vs: DistilBART, T5 (Holm p<.05)\n\n=== Suggested sentences ===\n- DistilBART vs LED on BERTScoreF1: worse; paired t p=0.0000, Wilcoxon p=0.0001, Holm-adjusted p=0.0002, d=-0.66 (n=48).\n- LED vs T5 on BERTScoreF1: better; paired t p=0.0002, Wilcoxon p=0.0002, Holm-adjusted p=0.0008, d=0.60 (n=48).\n- BART vs DistilBART on BERTScoreF1: better; paired t p=0.0080, Wilcoxon p=0.0024, Holm-adjusted p=0.0095, d=0.40 (n=48).\n- BART vs T5 on BERTScoreF1: better; paired t p=0.0036, Wilcoxon p=0.0034, Holm-adjusted p=0.0102, d=0.44 (n=48).\n- BART vs LED on BERTScoreF1: worse; paired t p=0.2350, Wilcoxon p=0.1113, Holm-adjusted p=0.2225, d=-0.17 (n=48).\n- DistilBART vs T5 on BERTScoreF1: worse; paired t p=0.8040, Wilcoxon p=0.8429, Holm-adjusted p=0.8040, d=-0.04 (n=48).\n- DistilBART vs LED on BLEU: worse; paired t p=0.0040, Wilcoxon p=0.0073, Holm-adjusted p=0.0240, d=-0.44 (n=48).\n- BART vs DistilBART on BLEU: better; paired t p=0.0120, Wilcoxon p=0.0452, Holm-adjusted p=0.0602, d=0.38 (n=48).\n- LED vs T5 on BLEU: better; paired t p=0.0151, Wilcoxon p=0.0213, Holm-adjusted p=0.0603, d=0.36 (n=48).\n- BART vs T5 on BLEU: better; paired t p=0.0258, Wilcoxon p=0.0254, Holm-adjusted p=0.0761, d=0.33 (n=48).\n- BART vs LED on BLEU: worse; paired t p=0.5094, Wilcoxon p=0.4762, Holm-adjusted p=0.9524, d=-0.10 (n=48).\n- DistilBART vs T5 on BLEU: worse; paired t p=0.7783, Wilcoxon p=0.6010, Holm-adjusted p=0.9524, d=-0.04 (n=48).\n- DistilBART vs LED on METEOR: worse; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=-0.95 (n=48).\n- LED vs T5 on METEOR: better; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=0.86 (n=48).\n- BART vs DistilBART on METEOR: better; paired t p=0.0005, Wilcoxon p=0.0004, Holm-adjusted p=0.0014, d=0.54 (n=48).\n- BART vs LED on METEOR: worse; paired t p=0.0067, Wilcoxon p=0.0097, Holm-adjusted p=0.0171, d=-0.41 (n=48).\n- BART vs T5 on METEOR: better; paired t p=0.0092, Wilcoxon p=0.0057, Holm-adjusted p=0.0171, d=0.39 (n=48).\n- DistilBART vs T5 on METEOR: worse; paired t p=0.1178, Wilcoxon p=0.0707, Holm-adjusted p=0.0707, d=-0.23 (n=48).\n- DistilBART vs LED on ROUGE1: worse; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=-1.02 (n=48).\n- LED vs T5 on ROUGE1: better; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=0.88 (n=48).\n- BART vs DistilBART on ROUGE1: better; paired t p=0.0000, Wilcoxon p=0.0001, Holm-adjusted p=0.0001, d=0.68 (n=48).\n- BART vs T5 on ROUGE1: better; paired t p=0.0005, Wilcoxon p=0.0006, Holm-adjusted p=0.0015, d=0.54 (n=48).\n- DistilBART vs T5 on ROUGE1: worse; paired t p=0.0458, Wilcoxon p=0.0465, Holm-adjusted p=0.0916, d=-0.30 (n=48).\n- BART vs LED on ROUGE1: worse; paired t p=0.3722, Wilcoxon p=0.2763, Holm-adjusted p=0.2763, d=-0.13 (n=48).\n- DistilBART vs LED on ROUGE2: worse; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=-0.86 (n=48).\n- LED vs T5 on ROUGE2: better; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=0.86 (n=48).\n- BART vs DistilBART on ROUGE2: better; paired t p=0.0074, Wilcoxon p=0.0188, Holm-adjusted p=0.0252, d=0.40 (n=48).\n- BART vs LED on ROUGE2: worse; paired t p=0.0063, Wilcoxon p=0.0133, Holm-adjusted p=0.0252, d=-0.41 (n=48).\n- BART vs T5 on ROUGE2: better; paired t p=0.0063, Wilcoxon p=0.0141, Holm-adjusted p=0.0252, d=0.41 (n=48).\n- DistilBART vs T5 on ROUGE2: worse; paired t p=0.6319, Wilcoxon p=0.7882, Holm-adjusted p=0.6319, d=-0.07 (n=48).\n- DistilBART vs LED on ROUGEL: worse; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=-1.11 (n=48).\n- LED vs T5 on ROUGEL: better; paired t p=0.0000, Wilcoxon p=0.0000, Holm-adjusted p=0.0000, d=1.03 (n=48).\n- BART vs DistilBART on ROUGEL: better; paired t p=0.0000, Wilcoxon p=0.0001, Holm-adjusted p=0.0002, d=0.66 (n=48).\n- BART vs LED on ROUGEL: worse; paired t p=0.0008, Wilcoxon p=0.0018, Holm-adjusted p=0.0023, d=-0.52 (n=48).\n- BART vs T5 on ROUGEL: better; paired t p=0.0019, Wilcoxon p=0.0009, Holm-adjusted p=0.0023, d=0.48 (n=48).\n- DistilBART vs T5 on ROUGEL: worse; paired t p=0.0470, Wilcoxon p=0.0522, Holm-adjusted p=0.0470, d=-0.29 (n=48).\n\n=== LaTeX blocks (copy one per metric if you want) ===\n\\begin{table}[ht]\\centering\n\\caption{Significance vs LED on ROUGE1}\n\\label{tab:sig_rouge1}\n\\begin{tabular}{lrrrrr}\\toprule\nModel & t p & Wilcoxon p & Holm p & d (paired) \\\\ \\midrule\nDistilBART & 0.0000 & 0.0000 & 0.0000 & -1.02 \\\\\nT5 & 0.0000 & 0.0000 & 0.0000 & 0.88 \\\\\nBART & 0.3722 & 0.2763 & 0.2763 & -0.13 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table} \n\n\\begin{table}[ht]\\centering\n\\caption{Significance vs LED on ROUGE2}\n\\label{tab:sig_rouge2}\n\\begin{tabular}{lrrrrr}\\toprule\nModel & t p & Wilcoxon p & Holm p & d (paired) \\\\ \\midrule\nDistilBART & 0.0000 & 0.0000 & 0.0000 & -0.86 \\\\\nT5 & 0.0000 & 0.0000 & 0.0000 & 0.86 \\\\\nBART & 0.0063 & 0.0133 & 0.0252 & -0.41 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table} \n\n\\begin{table}[ht]\\centering\n\\caption{Significance vs LED on ROUGEL}\n\\label{tab:sig_rougel}\n\\begin{tabular}{lrrrrr}\\toprule\nModel & t p & Wilcoxon p & Holm p & d (paired) \\\\ \\midrule\nDistilBART & 0.0000 & 0.0000 & 0.0000 & -1.11 \\\\\nT5 & 0.0000 & 0.0000 & 0.0000 & 1.03 \\\\\nBART & 0.0008 & 0.0018 & 0.0023 & -0.52 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table} \n\n\\begin{table}[ht]\\centering\n\\caption{Significance vs LED on BLEU}\n\\label{tab:sig_bleu}\n\\begin{tabular}{lrrrrr}\\toprule\nModel & t p & Wilcoxon p & Holm p & d (paired) \\\\ \\midrule\nDistilBART & 0.0040 & 0.0073 & 0.0240 & -0.44 \\\\\nT5 & 0.0151 & 0.0213 & 0.0603 & 0.36 \\\\\nBART & 0.5094 & 0.4762 & 0.9524 & -0.10 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table} \n\n\\begin{table}[ht]\\centering\n\\caption{Significance vs LED on METEOR}\n\\label{tab:sig_meteor}\n\\begin{tabular}{lrrrrr}\\toprule\nModel & t p & Wilcoxon p & Holm p & d (paired) \\\\ \\midrule\nDistilBART & 0.0000 & 0.0000 & 0.0000 & -0.95 \\\\\nT5 & 0.0000 & 0.0000 & 0.0000 & 0.86 \\\\\nBART & 0.0067 & 0.0097 & 0.0171 & -0.41 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table} \n\n\\begin{table}[ht]\\centering\n\\caption{Significance vs LED on BERTScoreF1}\n\\label{tab:sig_bertscoref1}\n\\begin{tabular}{lrrrrr}\\toprule\nModel & t p & Wilcoxon p & Holm p & d (paired) \\\\ \\midrule\nDistilBART & 0.0000 & 0.0001 & 0.0002 & -0.66 \\\\\nT5 & 0.0002 & 0.0002 & 0.0008 & 0.60 \\\\\nBART & 0.2350 & 0.1113 & 0.2225 & -0.17 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table} \n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/799151196.py:163: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  BEST_MODEL = summary_df.groupby('metric').apply(lambda g: g.iloc[0]['model']).to_dict()\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"BEST_MODEL = {m: 'LED' for m in METRICS}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T18:51:11.641356Z","iopub.execute_input":"2025-09-15T18:51:11.642597Z","iopub.status.idle":"2025-09-15T18:51:11.648145Z","shell.execute_reply.started":"2025-09-15T18:51:11.642551Z","shell.execute_reply":"2025-09-15T18:51:11.646971Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}